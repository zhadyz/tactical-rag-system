# Docker Compose file - Performance Optimized Configuration

services:
  
  # ============================================================
  # SERVICE 1: Ollama (the AI model server)
  # ============================================================
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    
    ports:
      - "11434:11434"
    
    volumes:
      - ollama-data:/root/.ollama
    
    restart: unless-stopped

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    environment:
      - OLLAMA_NUM_GPU=999
      - OLLAMA_GPU_LAYERS=999
      - OLLAMA_KEEP_ALIVE=-1  # Keep model loaded indefinitely (prevents VRAM recovery issues)
    
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/11434' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    
    networks:
      - rag-network
  
  # ============================================================
  # SERVICE 2: RAG Application - PERFORMANCE OPTIMIZED
  # ============================================================
  rag-app:
    build:
      context: .
      dockerfile: _config/Dockerfile
    
    container_name: rag-tactical-system
    
    runtime: nvidia
    
    ports:
      - "7860:7860"
    
    volumes:
      - ./documents:/app/documents
      - ./chroma_db:/app/chroma_db
      - ./logs:/app/logs
    
    environment:
      # Ollama connection
      - OLLAMA_HOST=http://ollama:11434
      - PYTHONUNBUFFERED=1
      
      # GPU settings
      - CUDA_VISIBLE_DEVICES=0
      - DEVICE_TYPE=cuda
      - USE_CUDA_DOCKER=true
      
      # PERFORMANCE OPTIMIZATIONS
      # Use quantized model for 30-40% speed boost
      - RAG_LLM__MODEL_NAME=llama3.1:8b
      
      # Reduce context window from 8192 to 4096 (30% faster)
      - RAG_LLM__NUM_CTX=4096
      
      # Cap temperature for more consistent, faster responses
      - RAG_LLM__TEMPERATURE=0.0
      
      # Limit max tokens to prevent rambling answers
      - RAG_LLM__MAX_TOKENS=300
      
      # Retrieval optimizations
      # Reduce final documents from 5 to 3
      - RAG_RETRIEVAL__FINAL_K=3
      
      # Reduce reranking candidates from 20 to 15
      - RAG_RETRIEVAL__RERANK_K=15
      
      # Chunking optimizations
      # Reduce chunk size from 800 to 500 tokens
      - RAG_CHUNKING__CHUNK_SIZE=500
      
      # Reduce overlap from 200 to 100
      - RAG_CHUNKING__CHUNK_OVERLAP=100
      
      # Performance settings
      # Enable batching for faster processing
      - RAG_PERFORMANCE__ENABLE_BATCHING=true
      - RAG_PERFORMANCE__MAX_BATCH_SIZE=16
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    depends_on:
      ollama:
        condition: service_healthy
    
    restart: unless-stopped
    
    networks:
      - rag-network

networks:
  rag-network:
    driver: bridge

volumes:
  ollama-data:
    driver: local