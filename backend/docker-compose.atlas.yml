# ============================================================
# ATLAS PROTOCOL - Production Docker Compose v4.0
# Optimized for RTX 5080 + 96GB RAM System
# ============================================================
# Services:
# - atlas-backend: FastAPI + RAG pipeline (48GB RAM, 14 cores, 15GB GPU)
# - qdrant: Vector database (16GB RAM, 4 cores)
# - redis: Cache layer (8GB RAM, 2 cores)
# - ollama: LLM inference (managed separately)
# - prometheus: Metrics collection
# - grafana: Monitoring dashboards
# - cadvisor: Container metrics
# - node-exporter: System metrics
# ============================================================

version: '3.8'

# ============================================================
# SHARED NETWORK
# ============================================================
networks:
  atlas-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

# ============================================================
# PERSISTENT VOLUMES
# ============================================================
volumes:
  qdrant_storage:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

# ============================================================
# SERVICES
# ============================================================
services:

  # ==========================================================
  # QDRANT VECTOR DATABASE
  # ==========================================================
  qdrant:
    image: qdrant/qdrant:v1.7.4
    container_name: atlas-qdrant
    restart: unless-stopped

    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC API

    volumes:
      - qdrant_storage:/qdrant/storage
      - ./data/qdrant:/qdrant/snapshots

    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__STORAGE__STORAGE_PATH=/qdrant/storage
      - QDRANT__STORAGE__SNAPSHOTS_PATH=/qdrant/snapshots
      - QDRANT__STORAGE__PERFORMANCE__MAX_SEARCH_THREADS=4
      - QDRANT__STORAGE__OPTIMIZERS__INDEXING_THRESHOLD=20000
      - QDRANT__STORAGE__WAL__WAL_CAPACITY_MB=128
      - QDRANT__LOG_LEVEL=INFO

    # Resource limits optimized for RTX 5080 system
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 16G
        reservations:
          cpus: '2'
          memory: 8G

    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:6333/healthz"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s

    networks:
      atlas-network:
        ipv4_address: 172.28.0.2

    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
        labels: "service=qdrant,environment=production"

  # ==========================================================
  # REDIS CACHE
  # ==========================================================
  redis:
    image: redis:7.2-alpine
    container_name: atlas-redis
    restart: unless-stopped

    ports:
      - "6379:6379"

    volumes:
      - redis_data:/data

    command: >
      redis-server
      --maxmemory 8gb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
      --appendonly yes
      --appendfsync everysec
      --loglevel notice

    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 8G
        reservations:
          cpus: '1'
          memory: 4G

    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s

    networks:
      atlas-network:
        ipv4_address: 172.28.0.3

    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
        labels: "service=redis,environment=production"

  # ==========================================================
  # ATLAS BACKEND (FastAPI + RAG Pipeline)
  # ==========================================================
  atlas-backend:
    build:
      context: ..
      dockerfile: backend/Dockerfile.atlas
      args:
        - BUILDKIT_INLINE_CACHE=1

    image: atlas-protocol-backend:v4.0
    container_name: atlas-backend
    restart: unless-stopped

    ports:
      - "8000:8000"

    volumes:
      - ../documents:/app/documents
      - ../models:/app/models
      - ../config.yml:/app/config.yml:ro
      - ./logs:/app/logs
      - type: tmpfs
        target: /tmp
        tmpfs:
          size: 4G

    environment:
      # Core Configuration
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - CONFIG_PATH=/app/config.yml

      # LLM Configuration
      - LLM_BACKEND=llamacpp
      - MODEL_PATH=/app/models/llama-3.1-8b-instruct.Q5_K_M.gguf
      - GPU_LAYERS=33
      - CONTEXT_LENGTH=8192
      - TEMPERATURE=0.7

      # Vector Store Configuration
      - VECTOR_STORE=qdrant
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_COLLECTION=atlas_knowledge_base
      - QDRANT_PREFER_GRPC=false

      # Cache Configuration
      - RAG_CACHE__USE_REDIS=true
      - RAG_CACHE__REDIS_HOST=redis
      - RAG_CACHE__REDIS_PORT=6379
      - RAG_CACHE__REDIS_DB=0
      - RAG_CACHE__TTL=3600

      # Embedding Configuration
      - EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
      - EMBEDDING_DIMENSION=1024
      - EMBEDDING_BATCH_SIZE=32

      # Reranker Configuration
      - RERANKER_MODEL=BAAI/bge-reranker-large
      - RERANKER_TOP_K=10

      # Document Processing
      - RAG_DOCUMENTS_DIR=/app/documents
      - CHUNK_SIZE=1024
      - CHUNK_OVERLAP=128

      # Performance
      - WORKERS=4
      - MAX_CONCURRENT_REQUESTS=100
      - REQUEST_TIMEOUT=120

      # Monitoring
      - ENABLE_METRICS=true
      - METRICS_PORT=8000
      - LOG_LEVEL=INFO
      - LOG_FORMAT=json

    depends_on:
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy

    # Resource limits for RTX 5080 + 96GB RAM
    deploy:
      resources:
        limits:
          cpus: '14'
          memory: 48G
        reservations:
          cpus: '8'
          memory: 24G

    # GPU passthrough for llama.cpp
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 60s

    networks:
      atlas-network:
        ipv4_address: 172.28.0.10

    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
        labels: "service=atlas-backend,environment=production"

  # ==========================================================
  # PROMETHEUS (Metrics Collection)
  # ==========================================================
  prometheus:
    image: prom/prometheus:v2.48.1
    container_name: atlas-prometheus
    restart: unless-stopped

    ports:
      - "9090:9090"

    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus

    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--storage.tsdb.retention.size=10GB'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'

    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G

    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 15s
      timeout: 5s
      retries: 3

    networks:
      atlas-network:
        ipv4_address: 172.28.0.20

    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # ==========================================================
  # GRAFANA (Visualization & Dashboards)
  # ==========================================================
  grafana:
    image: grafana/grafana:10.2.3
    container_name: atlas-grafana
    restart: unless-stopped

    ports:
      - "3001:3000"

    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro

    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=atlas_admin_2025
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3001
      - GF_INSTALL_PLUGINS=redis-datasource,grafana-clock-panel
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/var/lib/grafana/dashboards/atlas-overview.json

    depends_on:
      - prometheus

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 15s
      timeout: 5s
      retries: 3

    networks:
      atlas-network:
        ipv4_address: 172.28.0.21

    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # ==========================================================
  # cAdvisor (Container Metrics)
  # ==========================================================
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: atlas-cadvisor
    restart: unless-stopped

    ports:
      - "8081:8080"

    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /dev/disk:/dev/disk:ro

    privileged: true

    devices:
      - /dev/kmsg

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

    networks:
      atlas-network:
        ipv4_address: 172.28.0.22

    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # ==========================================================
  # Node Exporter (System Metrics)
  # ==========================================================
  node-exporter:
    image: prom/node-exporter:v1.7.0
    container_name: atlas-node-exporter
    restart: unless-stopped

    ports:
      - "9100:9100"

    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro

    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'

    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

    networks:
      atlas-network:
        ipv4_address: 172.28.0.23

    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

# ============================================================
# RESOURCE ALLOCATION SUMMARY (RTX 5080 + 96GB RAM)
# ============================================================
# Total System: 96GB RAM, 16 CPU cores, 16GB GPU VRAM
#
# Allocated:
# - ATLAS Backend:  48GB RAM, 14 cores, 15GB GPU (primary workload)
# - Qdrant:         16GB RAM,  4 cores (vector search)
# - Redis:           8GB RAM,  2 cores (cache)
# - Prometheus:      4GB RAM,  2 cores (metrics)
# - Grafana:         2GB RAM,  1 core  (dashboards)
# - cAdvisor:        2GB RAM,  1 core  (container metrics)
# - Node Exporter: 512MB RAM,  0.5 core (system metrics)
# - System Reserve: 15GB RAM,  3 cores (OS + buffer)
# ============================================================
# Total Allocated: ~81GB RAM, ~24.5 cores (with reservations)
# Headroom:        ~15GB RAM,  buffer for spikes
# ============================================================
