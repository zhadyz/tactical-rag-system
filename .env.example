# =============================================================================
# Tactical RAG Environment Configuration Template
# =============================================================================
# Copy this file to .env and update values for your environment
# cp .env.example .env

# =============================================================================
# SERVICE ENDPOINTS
# =============================================================================

# Ollama LLM Server
OLLAMA_HOST=http://localhost:11434
# Docker: http://ollama:11434
# Local: http://localhost:11434

# Redis Cache Server
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
# Docker: redis
# Local: localhost

# =============================================================================
# FILE PATHS & STORAGE
# =============================================================================

# Document storage directory
DOCUMENTS_DIR=./documents

# Vector database persistence directory
VECTOR_DB_DIR=./chroma_db
CHROMA_PERSIST_DIRECTORY=./chroma_db

# Cache directory
CACHE_DIR=./.cache

# Log directory
LOG_DIR=./logs

# =============================================================================
# API CONFIGURATION
# =============================================================================

# Backend API settings
API_HOST=0.0.0.0
API_PORT=8000

# CORS allowed origins (comma-separated)
# Development: Allow local dev servers
CORS_ORIGINS=http://localhost:3000,http://localhost:5173,http://127.0.0.1:3000,http://127.0.0.1:5173
# Production: Restrict to production domains
# CORS_ORIGINS=https://tacticalrag.yourdomain.com

# =============================================================================
# GPU & CUDA SETTINGS
# =============================================================================

# GPU device ID (0 for first GPU, -1 for CPU)
CUDA_VISIBLE_DEVICES=0

# Device type: cuda or cpu
DEVICE_TYPE=cuda

# Enable CUDA in Docker
USE_CUDA_DOCKER=true

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# LLM Model
LLM_MODEL=llama3.1:8b
# Options: llama3.1:8b, llama3.1:70b, mistral:7b, etc.

# Embedding Model
EMBEDDING_MODEL=nomic-embed-text
# Options: nomic-embed-text, all-minilm, etc.

# Model parameters
LLM_TEMPERATURE=0.0
LLM_TOP_P=0.9
LLM_TOP_K=40
LLM_NUM_CTX=8192
LLM_REPEAT_PENALTY=1.1
LLM_TIMEOUT=120

# =============================================================================
# RETRIEVAL CONFIGURATION
# =============================================================================

# Number of results to retrieve
INITIAL_K=50
RERANK_K=20
FINAL_K=5

# Simple mode K value
SIMPLE_K=5

# Hybrid search weights
DENSE_WEIGHT=0.5
SPARSE_WEIGHT=0.5

# Fusion method: rrf, weighted, score_normalization
FUSION_METHOD=rrf
RRF_CONSTANT=60

# Query complexity thresholds
SIMPLE_THRESHOLD=1
MODERATE_THRESHOLD=3

# =============================================================================
# CHUNKING CONFIGURATION
# =============================================================================

# Chunking strategy: recursive, semantic, sentence, hybrid
CHUNKING_STRATEGY=hybrid

# Chunk size and overlap
CHUNK_SIZE=800
CHUNK_OVERLAP=200
MIN_CHUNK_SIZE=100

# Semantic similarity threshold
SEMANTIC_SIMILARITY_THRESHOLD=0.7

# =============================================================================
# CACHING CONFIGURATION
# =============================================================================

# Enable caching layers
ENABLE_EMBEDDING_CACHE=true
ENABLE_QUERY_CACHE=true
ENABLE_RESULT_CACHE=true

# Redis caching
USE_REDIS=true

# Cache TTL in seconds
CACHE_TTL=3600

# Maximum cache entries
MAX_CACHE_SIZE=10000

# =============================================================================
# MONITORING & LOGGING
# =============================================================================

# Environment: development, staging, production
ENVIRONMENT=development

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Log file path
LOG_FILE=logs/rag_system.log

# Enable metrics
ENABLE_METRICS=true
METRICS_PORT=9090

# Enable tracing
ENABLE_TRACING=false
TRACE_SAMPLE_RATE=0.1

# =============================================================================
# PERFORMANCE OPTIMIZATION
# =============================================================================

# Thread pool size
MAX_WORKERS=4

# Enable async operations
ENABLE_ASYNC=true

# HTTP connection pool size
CONNECTION_POOL_SIZE=10

# Batching settings
ENABLE_BATCHING=true
BATCH_TIMEOUT_MS=100
MAX_BATCH_SIZE=32

# =============================================================================
# VECTOR STORE CONFIGURATION
# =============================================================================

# Vector store backend: chromadb (default) or qdrant
# ChromaDB: File-based, zero config, best for <100K vectors
# Qdrant: Server-based, production-grade, best for 500K+ vectors
USE_QDRANT=false

# Qdrant Configuration (only used if USE_QDRANT=true)
QDRANT_HOST=localhost
# Docker: qdrant
# Local: localhost

QDRANT_PORT=6333
QDRANT_COLLECTION=air_force_docs

# Qdrant gRPC port (optional, for better performance)
QDRANT_GRPC_PORT=6334
QDRANT_PREFER_GRPC=false

# Qdrant API key (optional, for Qdrant Cloud)
# QDRANT_API_KEY=your-api-key-here

# Qdrant HNSW index parameters
QDRANT_HNSW_M=16
QDRANT_HNSW_EF_CONSTRUCT=200

# Enable quantization (4x memory reduction)
QDRANT_ENABLE_QUANTIZATION=true

# =============================================================================
# LLM BACKEND CONFIGURATION (CRITICAL - 10-20x SPEEDUP)
# =============================================================================

# LLM Backend: ollama (16s response) or vllm (1-2s response)
# FALSE = Ollama (baseline, compatible, slower)
# TRUE = vLLM (10-20x speedup, production-ready)
USE_VLLM=false

# vLLM Configuration (only used if USE_VLLM=true)
VLLM_HOST=http://localhost:8001
# Docker: http://vllm-server:8000
# Local: http://localhost:8001

VLLM_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
# Options: meta-llama/Meta-Llama-3.1-8B-Instruct, mistralai/Mistral-7B-Instruct-v0.2, etc.

# vLLM Performance Settings
VLLM_TIMEOUT=90
VLLM_MAX_TOKENS=512
VLLM_RETRY_ATTEMPTS=3

# =============================================================================
# FEATURE FLAGS
# =============================================================================

# Enable evaluation metrics
ENABLE_EVALUATION=true

# Enable feedback loop
ENABLE_FEEDBACK_LOOP=true

# Enable auto-optimization
ENABLE_AUTO_OPTIMIZATION=false

# Enable conversation memory
ENABLE_CONVERSATION_MEMORY=true
MAX_CONVERSATION_EXCHANGES=10

# =============================================================================
# FRONTEND CONFIGURATION
# =============================================================================

# Frontend API URL
VITE_API_URL=http://localhost:8000

# Node environment
NODE_ENV=development

# =============================================================================
# SECURITY (Optional)
# =============================================================================

# API Key for authentication (optional, for future use)
# API_KEY=your-secret-api-key-here

# JWT Secret (optional, for future auth)
# JWT_SECRET=your-jwt-secret-here
# JWT_ALGORITHM=HS256
# JWT_EXPIRATION=3600

# =============================================================================
# DOCKER SPECIFIC SETTINGS
# =============================================================================

# When running in Docker, use these overrides:
# OLLAMA_HOST=http://ollama:11434
# REDIS_HOST=redis
# DOCUMENTS_DIR=/app/documents
# VECTOR_DB_DIR=/app/chroma_db
# LOG_DIR=/app/logs

# =============================================================================
# DEVELOPMENT vs PRODUCTION
# =============================================================================

# Development Configuration:
# - ENVIRONMENT=development
# - LOG_LEVEL=DEBUG
# - CORS_ORIGINS=http://localhost:3000,http://localhost:5173
# - ENABLE_TRACING=true

# Production Configuration:
# - ENVIRONMENT=production
# - LOG_LEVEL=INFO
# - CORS_ORIGINS=https://your-domain.com
# - ENABLE_TRACING=false
# - CACHE_TTL=7200 (longer TTL)
# - MAX_CACHE_SIZE=50000 (larger cache)

# =============================================================================
# NOTES
# =============================================================================

# 1. Never commit .env file to version control
# 2. Add .env to .gitignore
# 3. Keep .env.example up to date with latest configuration options
# 4. Use different .env files for different environments:
#    - .env.development
#    - .env.staging
#    - .env.production
# 5. Validate all required environment variables on application startup
