# Enterprise RAG System Configuration
# Customize these settings for your deployment

# System Paths
documents_dir: "./documents"
vector_db_dir: "./chroma_db"
cache_dir: "./.cache"

# Service Endpoints
# FIXED: Use Docker service name instead of localhost
ollama_host: "http://ollama:11434"
api_host: "0.0.0.0"
api_port: 7860

# Embedding Configuration
# CRITICAL: Must match the model used during database indexing!
# Database was indexed with: BAAI/bge-large-en-v1.5 (1024-dim, HuggingFace)
embedding:
  model_name: "BAAI/bge-large-en-v1.5"
  model_type: "huggingface"
  dimension: 1024
  batch_size: 64  # Was 32 - AGGRESSIVE batching (we have 96GB RAM!)
  cache_embeddings: true

# LLM Configuration
# Using Qwen 2.5 14B for superior reasoning and answer quality
llm:
  model_name: "qwen2.5:14b-instruct-q4_K_M"
  temperature: 0.0
  top_p: 0.9
  top_k: 40
  num_ctx: 8192
  repeat_penalty: 1.1
  timeout: 120

# Chunking Strategy
chunking:
  # Options: "recursive", "semantic", "sentence", "hybrid"
  strategy: "hybrid"
  chunk_size: 800
  chunk_overlap: 200
  min_chunk_size: 100
  semantic_similarity_threshold: 0.7

# Advanced Retrieval - OPTIMIZED FOR ACCURACY
retrieval:
  # Multi-stage retrieval parameters - INCREASED FOR BETTER ACCURACY
  initial_k: 100  # Was 50 - More candidates = better recall (we have RAM!)
  rerank_k: 30   # Was 20 - More reranking = better precision
  final_k: 8     # Was 5 - More context for LLM (better answers)

  # Hybrid search weights (must sum to 1.0)
  dense_weight: 0.6  # Was 0.5 - Favor semantic search slightly
  sparse_weight: 0.4  # Was 0.5 - Balanced approach

  # Fusion method: "rrf", "weighted", "score_normalization"
  fusion_method: "rrf"
  rrf_k: 60

  # Query enhancement - ALL ENABLED for max accuracy
  use_query_expansion: true
  use_query_decomposition: true
  use_hypothetical_document: true

  # Reranking - Keep existing model (good balance)
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-12-v2"
  use_colbert_rerank: false

  # Filtering - RELAXED for better recall
  use_metadata_filtering: true
  relevance_threshold: 0.2  # Was 0.3 - Lower threshold = more candidates

  # Multi-Query Fusion with HyDE (Hypothetical Document Embeddings)
  # Gold standard for handling vague/ambiguous queries at scale
  use_multi_query_fusion: true  # Enable/disable multi-query mode
  num_query_variants: 4  # Number of query variants to generate (3-5 recommended)
  fusion_algorithm: "reciprocal_rank_fusion"  # Options: "reciprocal_rank_fusion", "weighted_sum"
  min_query_length: 5  # Only use multi-query for queries with <N words (vague queries)
  max_query_length: 15  # Disable for very specific queries (>N words)

# Caching
cache:
  enable_embedding_cache: true
  enable_query_cache: true
  enable_result_cache: true
  
  cache_ttl: 3600  # seconds
  max_cache_size: 10000
  
  # Redis (optional - for distributed deployment)
  use_redis: false
  redis_host: "localhost"
  redis_port: 6379
  redis_db: 0

# Monitoring
monitoring:
  enable_logging: true
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  log_file: "logs/rag_system.log"
  
  enable_metrics: true
  metrics_port: 9090
  
  enable_tracing: false
  trace_sample_rate: 0.1

# Performance - AGGRESSIVE RESOURCE UTILIZATION
performance:
  max_workers: 16  # Was 4 - Use ALL CPU cores (96GB RAM!)
  enable_async: true
  connection_pool_size: 20  # Was 10 - More connections

  # Batching - AGGRESSIVE for high throughput
  enable_batching: true
  batch_timeout_ms: 50  # Was 100 - Faster batching
  max_batch_size: 128  # Was 32 - LARGE batches (RAM is not a constraint)

# Feature Flags
enable_evaluation: true
enable_feedback_loop: true
enable_auto_optimization: false