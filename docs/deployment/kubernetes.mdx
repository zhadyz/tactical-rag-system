# Kubernetes Deployment Guide

Deploy ATLAS Protocol on Kubernetes for enterprise-scale, high-availability RAG infrastructure.

---

## Prerequisites

- Kubernetes 1.25+
- Helm 3.10+
- GPU-enabled nodes (NVIDIA device plugin)
- 200GB+ persistent storage
- LoadBalancer or Ingress controller

---

## Quick Deploy (Helm Chart)

```bash
# Add ATLAS helm repository
helm repo add atlas https://charts.apollo.onyxlab.ai
helm repo update

# Install with default values
helm install atlas-rag atlas/atlas-protocol \
  --namespace atlas \
  --create-namespace

# Check deployment status
kubectl get pods -n atlas

# Get service URL
kubectl get svc -n atlas atlas-backend
```

---

## Helm Chart Structure

```
charts/atlas-protocol/
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ values.yaml
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ backend-deployment.yaml
â”‚   â”œâ”€â”€ backend-service.yaml
â”‚   â”œâ”€â”€ qdrant-statefulset.yaml
â”‚   â”œâ”€â”€ qdrant-service.yaml
â”‚   â”œâ”€â”€ redis-deployment.yaml
â”‚   â”œâ”€â”€ redis-service.yaml
â”‚   â”œâ”€â”€ ingress.yaml
â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”œâ”€â”€ secrets.yaml
â”‚   â”œâ”€â”€ pvc.yaml
â”‚   â”œâ”€â”€ serviceaccount.yaml
â”‚   â”œâ”€â”€ hpa.yaml
â”‚   â””â”€â”€ gpu-node-selector.yaml
â””â”€â”€ README.md
```

---

## Custom values.yaml

```yaml
# values.yaml - Production configuration

# Global settings
global:
  namespace: atlas
  storageClass: "fast-ssd"  # Use high-performance storage

# Backend configuration
backend:
  replicaCount: 3
  image:
    repository: atlas-protocol-backend
    tag: v4.0.0
    pullPolicy: IfNotPresent

  resources:
    limits:
      nvidia.com/gpu: 1
      cpu: "14"
      memory: "48Gi"
    requests:
      nvidia.com/gpu: 1
      cpu: "8"
      memory: "24Gi"

  # GPU node affinity
  nodeSelector:
    accelerator: nvidia-gpu
    gpu-model: rtx-4090  # or rtx-5080, a100, etc.

  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

  # Environment variables
  env:
    - name: LLM_BACKEND
      value: "llamacpp"
    - name: MODEL_PATH
      value: "/models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf"
    - name: GPU_LAYERS
      value: "33"
    - name: CONTEXT_LENGTH
      value: "8192"
    - name: VECTOR_STORE
      value: "qdrant"
    - name: QDRANT_HOST
      value: "atlas-qdrant"
    - name: REDIS_HOST
      value: "atlas-redis"

  # Readiness probe
  readinessProbe:
    httpGet:
      path: /api/health
      port: 8000
    initialDelaySeconds: 60
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 3

  # Liveness probe
  livenessProbe:
    httpGet:
      path: /api/health
      port: 8000
    initialDelaySeconds: 90
    periodSeconds: 30
    timeoutSeconds: 10
    failureThreshold: 3

  # Persistent volumes
  volumes:
    - name: models
      persistentVolumeClaim:
        claimName: atlas-models-pvc
    - name: documents
      persistentVolumeClaim:
        claimName: atlas-documents-pvc

  volumeMounts:
    - name: models
      mountPath: /models
      readOnly: true
    - name: documents
      mountPath: /app/documents

# Qdrant configuration
qdrant:
  enabled: true
  replicaCount: 1  # Single instance (scale with sharding later)

  image:
    repository: qdrant/qdrant
    tag: v1.15.0

  resources:
    limits:
      cpu: "4"
      memory: "16Gi"
    requests:
      cpu: "2"
      memory: "8Gi"

  persistence:
    enabled: true
    size: 100Gi
    storageClass: "fast-ssd"

  service:
    type: ClusterIP
    httpPort: 6333
    grpcPort: 6334

# Redis configuration
redis:
  enabled: true
  image:
    repository: redis
    tag: 7.2-alpine

  resources:
    limits:
      cpu: "2"
      memory: "8Gi"
    requests:
      cpu: "1"
      memory: "4Gi"

  persistence:
    enabled: true
    size: 20Gi

  config:
    maxmemory: "8gb"
    maxmemory-policy: "allkeys-lru"
    save: "900 1 300 10"

# Ingress configuration
ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "50m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "120"

  hosts:
    - host: apollo.onyxlab.ai
      paths:
        - path: /
          pathType: Prefix

  tls:
    - secretName: apollo-tls
      hosts:
        - apollo.onyxlab.ai

# Horizontal Pod Autoscaler
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# Monitoring
monitoring:
  prometheus:
    enabled: true
    serviceMonitor: true
  grafana:
    enabled: true
    dashboards: true
```

---

## GPU Node Configuration

### NVIDIA Device Plugin

```bash
# Install NVIDIA device plugin
kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.0/nvidia-device-plugin.yml

# Verify GPU nodes
kubectl get nodes -o json | jq '.items[].status.allocatable."nvidia.com/gpu"'
```

### Label GPU Nodes

```bash
# Label nodes with GPU info
kubectl label nodes k8s-gpu-node-1 \
  accelerator=nvidia-gpu \
  gpu-model=rtx-4090 \
  gpu-memory=24gb

# Verify labels
kubectl describe node k8s-gpu-node-1 | grep -A5 Labels
```

---

## Persistent Volumes

### Storage Class (NVMe SSD)

```yaml
# storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs  # or kubernetes.io/gce-pd
parameters:
  type: gp3
  iops: "10000"
  throughput: "500"
  fsType: ext4
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
```

### PersistentVolumeClaim: Models

```yaml
# pvc-models.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: atlas-models-pvc
  namespace: atlas
spec:
  accessModes:
    - ReadOnlyMany   # Multiple pods read same models
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 50Gi
```

### PersistentVolumeClaim: Documents

```yaml
# pvc-documents.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: atlas-documents-pvc
  namespace: atlas
spec:
  accessModes:
    - ReadWriteMany  # Multiple pods upload documents
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 100Gi
```

### PersistentVolumeClaim: Qdrant

```yaml
# pvc-qdrant.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: atlas-qdrant-pvc
  namespace: atlas
spec:
  accessModes:
    - ReadWriteOnce  # Single Qdrant instance
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 100Gi
```

---

## Deployment Manifests

### Backend Deployment

```yaml
# backend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: atlas-backend
  namespace: atlas
spec:
  replicas: 3
  selector:
    matchLabels:
      app: atlas-backend
  template:
    metadata:
      labels:
        app: atlas-backend
    spec:
      nodeSelector:
        accelerator: nvidia-gpu

      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

      containers:
        - name: backend
          image: atlas-protocol-backend:v4.0.0
          imagePullPolicy: IfNotPresent

          ports:
            - containerPort: 8000
              name: http

          env:
            - name: LLM_BACKEND
              value: "llamacpp"
            - name: MODEL_PATH
              value: "/models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf"
            - name: GPU_LAYERS
              value: "33"
            - name: QDRANT_HOST
              value: "atlas-qdrant"
            - name: REDIS_HOST
              value: "atlas-redis"

          resources:
            limits:
              nvidia.com/gpu: 1
              cpu: "14"
              memory: "48Gi"
            requests:
              nvidia.com/gpu: 1
              cpu: "8"
              memory: "24Gi"

          volumeMounts:
            - name: models
              mountPath: /models
              readOnly: true
            - name: documents
              mountPath: /app/documents
            - name: config
              mountPath: /app/config.yml
              subPath: config.yml

          readinessProbe:
            httpGet:
              path: /api/health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 10

          livenessProbe:
            httpGet:
              path: /api/health
              port: 8000
            initialDelaySeconds: 90
            periodSeconds: 30

      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: atlas-models-pvc
        - name: documents
          persistentVolumeClaim:
            claimName: atlas-documents-pvc
        - name: config
          configMap:
            name: atlas-config
```

### Backend Service

```yaml
# backend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: atlas-backend
  namespace: atlas
spec:
  type: ClusterIP
  selector:
    app: atlas-backend
  ports:
    - port: 8000
      targetPort: 8000
      protocol: TCP
      name: http
```

---

### Qdrant StatefulSet

```yaml
# qdrant-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: atlas-qdrant
  namespace: atlas
spec:
  serviceName: atlas-qdrant
  replicas: 1
  selector:
    matchLabels:
      app: atlas-qdrant

  template:
    metadata:
      labels:
        app: atlas-qdrant
    spec:
      containers:
        - name: qdrant
          image: qdrant/qdrant:v1.15.0

          ports:
            - containerPort: 6333
              name: http
            - containerPort: 6334
              name: grpc

          env:
            - name: QDRANT__SERVICE__HTTP_PORT
              value: "6333"
            - name: QDRANT__SERVICE__GRPC_PORT
              value: "6334"

          resources:
            limits:
              cpu: "4"
              memory: "16Gi"
            requests:
              cpu: "2"
              memory: "8Gi"

          volumeMounts:
            - name: qdrant-storage
              mountPath: /qdrant/storage

  volumeClaimTemplates:
    - metadata:
        name: qdrant-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "fast-ssd"
        resources:
          requests:
            storage: 100Gi
```

---

## Horizontal Pod Autoscaler (HPA)

```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: atlas-backend-hpa
  namespace: atlas
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: atlas-backend

  minReplicas: 2
  maxReplicas: 10

  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

    - type: Pods
      pods:
        metric:
          name: atlas_query_duration_seconds
        target:
          type: AverageValue
          averageValue: "2"  # Scale if avg query > 2s

  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5min before scale-down
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60

    scaleUp:
      stabilizationWindowSeconds: 0  # Immediate scale-up
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
```

**Autoscaling Logic:**
- CPU > 70% â†’ Add pod
- Memory > 80% â†’ Add pod
- Query latency > 2s â†’ Add pod
- Scale down slowly (prevent thrashing)
- Scale up quickly (handle traffic spikes)

---

## Ingress Configuration

```yaml
# ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: atlas-ingress
  namespace: atlas
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "50m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "120"
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/cors-allow-origin: "*"

spec:
  ingressClassName: nginx

  tls:
    - hosts:
        - apollo.onyxlab.ai
      secretName: apollo-tls

  rules:
    - host: apollo.onyxlab.ai
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: atlas-backend
                port:
                  number: 8000
```

---

## Secrets Management

### Create Secrets

```bash
# Redis password
kubectl create secret generic atlas-redis-password \
  --from-literal=password='SuperSecurePassword123!' \
  -n atlas

# API keys (if using external LLM)
kubectl create secret generic atlas-api-keys \
  --from-literal=openai-key='sk-...' \
  --from-literal=anthropic-key='sk-ant-...' \
  -n atlas

# TLS certificate (if not using cert-manager)
kubectl create secret tls apollo-tls \
  --cert=path/to/tls.crt \
  --key=path/to/tls.key \
  -n atlas
```

### Use Secrets in Deployment

```yaml
env:
  - name: REDIS_PASSWORD
    valueFrom:
      secretKeyRef:
        name: atlas-redis-password
        key: password

  - name: OPENAI_API_KEY
    valueFrom:
      secretKeyRef:
        name: atlas-api-keys
        key: openai-key
```

---

## ConfigMap for config.yml

```yaml
# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: atlas-config
  namespace: atlas
data:
  config.yml: |
    llamacpp:
      model_path: "/models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf"
      n_gpu_layers: 33
      n_ctx: 8192
      n_batch: 512
      temperature: 0.0
      enable_speculative_decoding: true
      draft_model_path: "/models/Llama-3.2-1B-Instruct.Q4_K_M.gguf"

    advanced_reranking:
      enable_llm_reranking: true
      llm_rerank_top_n: 3
      rerank_preset: "quality"

    cache:
      redis_host: "atlas-redis"
      redis_port: 6379
      ttl: 604800
```

---

## Production Checklist

### Pre-Deployment

- [ ] GPU nodes labeled and available
- [ ] Persistent volumes provisioned
- [ ] Models uploaded to PVC
- [ ] Secrets created
- [ ] Ingress controller configured
- [ ] Cert-manager installed (for TLS)
- [ ] NVIDIA device plugin running

### Post-Deployment

- [ ] All pods in `Running` state
- [ ] Health checks passing
- [ ] Ingress reachable externally
- [ ] TLS certificate valid
- [ ] Metrics scraping (Prometheus)
- [ ] Dashboards accessible (Grafana)
- [ ] HPA triggering correctly
- [ ] Backup strategy implemented

---

## Monitoring & Logging

### Prometheus ServiceMonitor

```yaml
# servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: atlas-backend-monitor
  namespace: atlas
spec:
  selector:
    matchLabels:
      app: atlas-backend
  endpoints:
    - port: http
      path: /metrics
      interval: 15s
```

### Grafana Dashboards

Deploy via ConfigMap:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: atlas
data:
  atlas-overview.json: |
    { "dashboard": ... }
```

---

## Disaster Recovery

### Velero Backup

```bash
# Install Velero
velero install \
  --provider aws \
  --bucket atlas-backups \
  --secret-file ./credentials-velero

# Create scheduled backup
velero schedule create atlas-daily \
  --schedule="0 2 * * *" \
  --include-namespaces atlas \
  --ttl 168h  # 7 days retention
```

### Manual Backup

```bash
# Backup Qdrant
kubectl exec -n atlas atlas-qdrant-0 -- \
  curl -X POST http://localhost:6333/collections/atlas_knowledge_base/snapshots

# Backup Redis
kubectl exec -n atlas atlas-redis-0 -- redis-cli SAVE

# Backup ConfigMaps & Secrets
kubectl get configmap,secret -n atlas -o yaml > backup-configs.yaml
```

---

## Troubleshooting

### Pods Pending (GPU)

```bash
# Check GPU availability
kubectl describe nodes | grep -A 10 Allocatable
```

**Fix**: Ensure NVIDIA device plugin is running.

### OOM Kills

```bash
# Check events
kubectl get events -n atlas --sort-by='.lastTimestamp'
```

**Fix**: Increase memory limits in values.yaml.

### PVC Not Binding

```bash
kubectl describe pvc atlas-models-pvc -n atlas
```

**Fix**: Check StorageClass availability.

---

## Next Steps

- ðŸ”§ [Model Management](../advanced/model-management.mdx)
- ðŸ“š [API Reference](../api/endpoints.mdx)
- ðŸ”’ [Security Best Practices](../advanced/security.mdx)

---

**Last Updated**: January 2025 | **Version**: 4.0.0
