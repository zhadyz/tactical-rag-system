# Model Management Guide

Master model selection, quantization, hot-swapping, and benchmarking for ATLAS Protocol.

---

## Supported Models

ATLAS Protocol supports any GGUF-format model via llama.cpp. Here are tested and recommended models:

### Llama 3.1 Family (Recommended)

| Model | Parameters | Quantization | VRAM | Speed | Accuracy | Use Case |
|-------|------------|--------------|------|-------|----------|----------|
| **Llama-3.1-8B-Instruct** | 8B | Q4_K_M | 5.5GB | ⚡⚡⚡⚡ | ⭐⭐⭐⭐ | Balanced, production |
| **Llama-3.1-8B-Instruct** | 8B | Q5_K_M | 6.5GB | ⚡⚡⚡ | ⭐⭐⭐⭐⭐ | **Default** |
| **Llama-3.1-8B-Instruct** | 8B | Q8_0 | 9.0GB | ⚡⚡ | ⭐⭐⭐⭐⭐ | Maximum quality |
| **Llama-3.1-70B-Instruct** | 70B | Q4_K_M | 42GB | ⚡ | ⭐⭐⭐⭐⭐ | Enterprise (A100) |

### Llama 3.2 Family (Lightweight)

| Model | Parameters | Quantization | VRAM | Speed | Accuracy | Use Case |
|-------|------------|--------------|------|-------|----------|----------|
| **Llama-3.2-1B-Instruct** | 1B | Q4_K_M | 771MB | ⚡⚡⚡⚡⚡ | ⭐⭐⭐ | Draft model, edge devices |
| **Llama-3.2-3B-Instruct** | 3B | Q4_K_M | 2.0GB | ⚡⚡⚡⚡ | ⭐⭐⭐⭐ | Low VRAM, fast |

### Mistral Family

| Model | Parameters | Quantization | VRAM | Speed | Accuracy | Use Case |
|-------|------------|--------------|------|-------|----------|----------|
| **Mistral-7B-Instruct-v0.3** | 7B | Q5_K_M | 5.5GB | ⚡⚡⚡ | ⭐⭐⭐⭐ | Alternative to Llama |
| **Mixtral-8x7B-Instruct** | 47B | Q4_K_M | 26GB | ⚡⚡ | ⭐⭐⭐⭐⭐ | MoE, high quality |

### Specialized Models

| Model | Parameters | VRAM | Best For |
|-------|------------|------|----------|
| **Qwen2.5-7B-Instruct** | 7B | 5.5GB | Multilingual, coding |
| **Phi-3-mini-128k** | 3.8B | 2.5GB | Long context (128K tokens) |
| **DeepSeek-Coder-7B** | 7B | 5.5GB | Code generation, RAG over codebases |

---

## Quantization Explained

Quantization reduces model size and memory usage by storing weights with lower precision.

### Quantization Methods

| Method | Bits | Size Reduction | Quality Loss | Speed | Recommended For |
|--------|------|----------------|--------------|-------|-----------------|
| **Q4_K_M** | 4-bit | ~70% | ~2% | Fastest | Production, low VRAM |
| **Q5_K_M** | 5-bit | ~60% | ~1% | Fast | **Default**, balanced |
| **Q6_K** | 6-bit | ~50% | <1% | Medium | High quality needed |
| **Q8_0** | 8-bit | ~30% | <0.5% | Slower | Maximum accuracy |
| **F16** | 16-bit | ~0% | 0% | Slowest | Research only |

### Performance Comparison (Llama-3.1-8B on RTX 4090)

| Quantization | Size | VRAM | Tokens/s | Perplexity | File Size |
|--------------|------|------|----------|------------|-----------|
| Q4_K_M | 4.6GB | 5.5GB | 110 tok/s | 7.23 | 4.6GB |
| Q5_K_M | 5.4GB | 6.5GB | 95 tok/s | 7.05 | 5.4GB |
| Q6_K | 6.8GB | 8.0GB | 85 tok/s | 7.01 | 6.8GB |
| Q8_0 | 9.0GB | 10.5GB | 70 tok/s | 6.98 | 9.0GB |
| F16 | 15GB | 18GB | 55 tok/s | 6.96 | 15GB |

**Recommendation**: Use Q5_K_M for production (best quality/speed balance).

---

## Downloading Models

### Official HuggingFace Repositories

```bash
# Create models directory
mkdir -p models

# Install HuggingFace CLI
pip install huggingface-hub

# Download Llama 3.1 8B Q5_K_M (default)
huggingface-cli download \
  bartowski/Meta-Llama-3.1-8B-Instruct-GGUF \
  Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf \
  --local-dir models

# Download draft model for speculative decoding
huggingface-cli download \
  bartowski/Llama-3.2-1B-Instruct-GGUF \
  Llama-3.2-1B-Instruct-Q4_K_M.gguf \
  --local-dir models
```

### Alternative: Direct Download

```bash
# Llama 3.1 8B Q5_K_M
wget -O models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf \
  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf

# Llama 3.1 8B Q4_K_M (faster)
wget -O models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf \
  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf

# Mistral 7B v0.3
wget -O models/Mistral-7B-Instruct-v0.3-Q5_K_M.gguf \
  https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-Q5_K_M.gguf
```

---

## Model Hot-Swapping

Change models WITHOUT restarting containers or redeploying.

### Method 1: Update config.yml

```yaml
# Edit backend/config.yml
llamacpp:
  model_path: "./models/Mistral-7B-Instruct-v0.3-Q5_K_M.gguf"  # Changed
  n_gpu_layers: 33
  n_ctx: 8192
```

```bash
# Restart backend to load new model
docker compose -f backend/docker-compose.atlas.yml restart atlas-backend

# Verify new model loaded
curl http://localhost:8000/api/health | jq '.model_loaded'
```

### Method 2: Environment Variable Override

```bash
# Stop services
docker compose -f backend/docker-compose.atlas.yml down

# Edit docker-compose.atlas.yml
environment:
  - MODEL_PATH=/app/models/Llama-3.1-8B-Instruct-Q4_K_M.gguf  # Changed

# Start with new model
docker compose -f backend/docker-compose.atlas.yml up -d
```

### Method 3: API Reload (Planned Feature)

```bash
# Future: Reload model via API (no restart)
curl -X POST http://localhost:8000/api/admin/reload-model \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -d '{"model_path": "/models/new-model.gguf"}'
```

---

## Custom Model Integration

### 1. Convert PyTorch/Safetensors to GGUF

```bash
# Clone llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# Install Python dependencies
pip install -r requirements.txt

# Convert model (example: Llama-2-7B)
python3 convert.py /path/to/llama2-7b-hf \
  --outfile models/llama2-7b-f16.gguf \
  --outtype f16

# Quantize to Q5_K_M
./quantize models/llama2-7b-f16.gguf \
  models/llama2-7b-Q5_K_M.gguf \
  Q5_K_M
```

### 2. Test Model Compatibility

```python
# test_model.py
from llama_cpp import Llama

model_path = "./models/custom-model-Q5_K_M.gguf"

# Test loading
llm = Llama(
    model_path=model_path,
    n_gpu_layers=33,
    n_ctx=8192,
    verbose=True
)

# Test generation
response = llm("Hello, who are you?", max_tokens=128)
print(response["choices"][0]["text"])
```

### 3. Integrate into ATLAS

```yaml
# config.yml
llamacpp:
  model_path: "./models/custom-model-Q5_K_M.gguf"
  n_gpu_layers: 33
  n_ctx: 8192
  # Adjust parameters for your model's architecture
```

---

## Speculative Decoding Setup

Speculative decoding uses a small "draft" model to predict tokens ahead, then validates with the main model. **Result: 30-50% faster inference**.

### Requirements
- Main model (e.g., Llama-3.1-8B)
- Draft model (e.g., Llama-3.2-1B, same architecture)
- Compatible architectures (both Llama, both Mistral, etc.)

### Configuration

```yaml
llamacpp:
  # Main model
  model_path: "./models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf"
  n_gpu_layers: 33

  # Speculative decoding
  enable_speculative_decoding: true
  draft_model_path: "./models/Llama-3.2-1B-Instruct-Q4_K_M.gguf"
  n_gpu_layers_draft: 33   # Full GPU for draft too
  num_draft: 5             # Draft 5 tokens ahead
```

### Benchmarking Impact

| Configuration | Latency | Speedup |
|---------------|---------|---------|
| Without speculative decoding | 500ms | 1.0x (baseline) |
| With speculative decoding (num_draft=3) | 380ms | 1.3x |
| With speculative decoding (num_draft=5) | 300ms | 1.7x |
| With speculative decoding (num_draft=8) | 290ms | 1.7x (diminishing returns) |

**Recommendation**: `num_draft=5` for optimal balance.

---

## Memory Requirements

### VRAM Calculation

```
VRAM = Model Size + Context Buffer + Draft Model (if enabled)

Context Buffer = (n_ctx × n_batch × 2 bytes) / 1024³

Example (Llama-3.1-8B Q5_K_M):
- Model: 5.4GB
- Context (8192 ctx, 512 batch): (8192 × 512 × 2) / 1024³ ≈ 0.008GB
- Draft model: 0.77GB
- Total: ~6.2GB VRAM
```

### RAM Calculation

```
RAM = Embedding Model + Reranker + Vector Cache + OS

Example:
- Embedding (bge-large-en-v1.5): 1.3GB
- Reranker (bge-reranker-large): 1.2GB
- Vector cache: 2-4GB
- OS + buffers: 8GB
- Total: ~14GB RAM minimum
```

---

## Performance vs. Accuracy Trade-offs

### Scenario: Limited VRAM (8GB)

**Option 1: Smaller Model, High Quality**
```yaml
llamacpp:
  model_path: "./models/Llama-3.2-3B-Instruct-Q8_0.gguf"
  n_gpu_layers: 33
  n_ctx: 8192
```
- ✅ Fits in VRAM
- ✅ High quality (Q8_0)
- ❌ Smaller model (lower accuracy)

**Option 2: Larger Model, Lower Quality**
```yaml
llamacpp:
  model_path: "./models/Llama-3.1-8B-Instruct-Q4_K_M.gguf"
  n_gpu_layers: 33
  n_ctx: 8192
```
- ✅ Larger model (better accuracy)
- ✅ Fits in VRAM
- ❌ Lower quality quantization

**Recommendation**: Option 2 (larger model, Q4 quantization) usually performs better than smaller model with higher quantization.

---

## Benchmarking Models

### Automated Benchmark Script

```python
# benchmark.py
import requests
import time
from statistics import mean, stdev

def benchmark_query(question: str, iterations: int = 10):
    """Benchmark query performance"""
    latencies = []

    for _ in range(iterations):
        start = time.time()
        response = requests.post(
            "http://localhost:8000/api/query",
            json={"question": question, "mode": "simple"}
        )
        end = time.time()

        if response.status_code == 200:
            latencies.append((end - start) * 1000)  # ms

    return {
        "mean_latency_ms": mean(latencies),
        "std_dev_ms": stdev(latencies),
        "min_ms": min(latencies),
        "max_ms": max(latencies),
        "tokens_per_second": response.json()["metadata"]["tokens_per_second"]
    }

# Test queries
queries = [
    "What is the capital of France?",
    "Explain quantum computing in simple terms.",
    "Compare Python and JavaScript for web development."
]

for query in queries:
    result = benchmark_query(query)
    print(f"\nQuery: {query[:50]}...")
    print(f"Mean: {result['mean_latency_ms']:.2f}ms")
    print(f"Tokens/s: {result['tokens_per_second']:.1f}")
```

### Perplexity Evaluation

```bash
# Download evaluation dataset
wget https://huggingface.co/datasets/wikitext/resolve/main/wikitext-2-raw/test.txt

# Run perplexity test
./llama.cpp/perplexity \
  -m models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf \
  -f test.txt \
  -ngl 33
```

**Lower perplexity = Better model quality**

---

## Model Comparison Matrix

| Criteria | Llama-3.1-8B Q5 | Llama-3.2-3B Q4 | Mistral-7B Q5 | Mixtral-8x7B Q4 |
|----------|-----------------|-----------------|---------------|-----------------|
| **VRAM** | 6.5GB | 2.0GB | 5.5GB | 26GB |
| **Speed (tok/s)** | 95 | 140 | 100 | 45 |
| **RAG Accuracy** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **Context Limit** | 128K | 128K | 32K | 32K |
| **Multilingual** | Good | Good | Excellent | Excellent |
| **Code Generation** | Good | Fair | Good | Excellent |
| **Cost/Performance** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ |

**Winner for RAG**: Llama-3.1-8B Q5_K_M (default)

---

## Troubleshooting

### Issue: Model fails to load

```
FileNotFoundError: ./models/model.gguf not found
```

**Fix**:
```bash
# Verify file exists
ls -lh models/

# Check permissions
chmod 644 models/*.gguf

# Verify path in config.yml
model_path: "./models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf"
```

### Issue: CUDA out of memory

```
CUDA error: out of memory
```

**Fix**:
```yaml
# Reduce GPU layers
n_gpu_layers: 24  # From 33

# Or use smaller quantization
model_path: "./models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"

# Or reduce context
n_ctx: 4096  # From 8192
```

### Issue: Slow generation (<10 tok/s)

**Check**:
```bash
# Verify GPU is active
curl http://localhost:8000/api/health | jq '.gpu_available'
```

**Fix**:
```yaml
# Ensure GPU offload
n_gpu_layers: 33

# Increase batch size
n_batch: 1024  # From 512
```

---

## Next Steps

- 📚 [API Reference](../api/endpoints.mdx)
- ⚙️ [Configuration Guide](../getting-started/configuration.mdx)
- 🚀 [Performance Tuning](./performance-tuning.mdx)

---

**Last Updated**: January 2025 | **Version**: 4.0.0
