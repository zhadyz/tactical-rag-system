# Architecture Evolution: From Prototype to Production

<div className="evolution-intro">
How Apollo's architecture evolved through 6 major versions, each solving specific problems while maintaining backward compatibility and improving performance. This is the story of architectural decisions, their trade-offs, and the lessons learned.
</div>

## The Architecture Timeline

### v1.0 - Basic RAG (January 2024)

**Goal**: Prove the concept works.

```
┌─────────────────┐
│  User Query     │
└────────┬────────┘
         │
    ┌────▼──────────┐
    │  Chroma VectorDB │
    └────┬──────────┘
         │ Retrieve 10 chunks
    ┌────▼──────────┐
    │  Ollama LLM   │
    │  (Llama 7B)   │
    └────┬──────────┘
         │
    ┌────▼──────────┐
    │  Answer       │
    └───────────────┘
```

**Stack**:
- Vector DB: Chroma (in-memory)
- Embedding: all-MiniLM-L6-v2 (384-dim)
- LLM: Ollama (Llama 2 7B)
- Interface: FastAPI + React

**Performance**:
- Query time: 25-30 seconds
- Accuracy: 60%
- Memory: 8GB

**Limitations**:
- Single retrieval strategy (dense-only)
- No reranking (quality issues)
- No caching (every query slow)
- Memory-only storage (no persistence)

**Why it worked**: Proved RAG could answer policy questions with citations.
**Why it failed**: Too slow and inaccurate for production.

---

### v2.0 - Hybrid Retrieval (March 2024)

**Goal**: Improve retrieval quality through hybrid search.

```
                ┌─────────────────┐
                │  User Query     │
                └────────┬────────┘
                         │
            ┌────────────┴────────────┐
            │                         │
    ┌───────▼────────┐      ┌────────▼───────┐
    │  Dense Search  │      │  BM25 Sparse   │
    │  (Chroma)      │      │  (Custom)      │
    └───────┬────────┘      └────────┬───────┘
            │                         │
            └────────────┬────────────┘
                         │ RRF Fusion
                    ┌────▼──────────┐
                    │  Top 10 Docs  │
                    └────┬──────────┘
                         │
                    ┌────▼──────────┐
                    │  Ollama LLM   │
                    └────┬──────────┘
                         │
                    ┌────▼──────────┐
                    │  Answer       │
                    └───────────────┘
```

**New Components**:
- BM25 sparse retrieval (keyword matching)
- RRF fusion (combine dense + sparse)
- Persistent storage (SQLite metadata)

**Architecture Decision #1**: Why RRF over learned fusion?

**Reasoning**:
- RRF is parameter-free (no training data needed)
- Formula: `score = Σ (1 / (k + rank_i))` where k=60
- Good baseline performance
- Can upgrade to learned fusion later

**Trade-off**: Acceptable. RRF achieved 72% accuracy (12% improvement) without training complexity.

---

**Performance**:
- Query time: 18-22 seconds
- Accuracy: 72% (+12% vs v1.0)
- Memory: 10GB

**Impact**: Hybrid search caught edge cases that pure semantic search missed.

**Example where hybrid helped**:
```
Query: "AFI 36-2903 section 3.1.2"
Dense search: Found semantic matches but missed exact section reference
BM25 search: Found exact "3.1.2" match
RRF fusion: Combined both → correct answer
```

---

### v3.0 - Advanced Reranking (June 2024)

**Goal**: Improve precision through two-stage reranking.

```
User Query
    │
    ├──> Dense Search (30 docs)
    ├──> BM25 Search (30 docs)
    │
    └──> RRF Fusion (deduplicate → 30 unique docs)
         │
         └──> Cross-Encoder Reranking (ms-marco-MiniLM-L-12-v2)
              │ Score all 30 docs
              └──> Top 10 docs
                   │
                   └──> LLM Reranking (score 1-10 for top 5)
                        │
                        └──> Top 5 final docs
                             │
                             └──> Answer Generation
```

**New Components**:
- Cross-encoder reranking (ms-marco-MiniLM-L-12-v2)
- LLM-based scoring (1-10 relevance scores)
- Two-stage pipeline (coarse → fine)

**Architecture Decision #2**: Why two stages instead of one reranker?

**Reasoning**:
- Cross-encoder: Fast but lower quality (150ms for 30 docs)
- LLM reranking: Slow but higher quality (3-4s for 5 docs)
- Two stages: Fast first pass, slow precision pass

**Cost-benefit**:
```
One-stage (LLM only):
  30 docs × 250ms = 7,500ms

Two-stage (Cross-encoder + LLM):
  Cross-encoder: 30 docs in 150ms (batch)
  LLM: 5 docs × 250ms = 1,250ms
  Total: 1,400ms

Savings: 6,100ms (81% faster)
Quality: Same final top-5 documents
```

**Trade-off**: Accepted. Two-stage reranking was 81% faster with same quality.

---

**Performance**:
- Query time: 15-20 seconds
- Accuracy: 81% (+9% vs v2.0)
- Memory: 12GB

**Bottleneck Identified**: LLM reranking still took 3-4 seconds. Room for optimization.

---

### v4.0 - Desktop Application (September 2024)

**Goal**: Native desktop experience with production-grade infrastructure.

```
┌──────────────────────────────────────────────────────┐
│         Tauri Desktop Shell (Rust)                   │
│  ┌────────────────────────────────────────────────┐  │
│  │      React Frontend (Embedded Webview)         │  │
│  │                                                │  │
│  │  ┌─────────┐  ┌──────────┐  ┌──────────────┐ │  │
│  │  │ Chat UI │  │ Document │  │ Performance  │ │  │
│  │  │         │  │ Manager  │  │ Dashboard    │ │  │
│  │  └─────────┘  └──────────┘  └──────────────┘ │  │
│  └──────────────────┬─────────────────────────────┘  │
│                     │ IPC (JSON-RPC)                 │
│  ┌──────────────────┴─────────────────────────────┐  │
│  │      Rust Backend Integration Layer            │  │
│  └──────────────────┬─────────────────────────────┘  │
└────────────────────┬───────────────────────────────┘
                     │ HTTP/REST
          ┌──────────┴──────────┐
          │                     │
    ┌─────▼────────┐     ┌──────▼──────────────┐
    │ Redis Cache  │     │ FastAPI Backend     │
    │              │     │  (Python 3.11)      │
    │ L1: Queries  │     │                     │
    │ L2: Vectors  │     │ ┌─────────────────┐ │
    └──────────────┘     │ │ Ollama LLM      │ │
                         │ │ (Llama 3.1 8B)  │ │
                         │ └─────────────────┘ │
                         │ ┌─────────────────┐ │
                         │ │ RAG Pipeline    │ │
                         │ │ - Hybrid search │ │
                         │ │ - 2-stage rerank│ │
                         │ └─────────────────┘ │
                         └──────┬──────────────┘
                                │
                         ┌──────▼──────────┐
                         │ Qdrant VectorDB │
                         │ (HNSW indexing) │
                         └─────────────────┘
```

**Major Changes**:

1. **Tauri Desktop Shell**
   - Cross-platform (Windows, macOS, Linux)
   - Native file dialogs
   - 50MB memory footprint
   - Security: Capability-based file access

2. **Qdrant Vector Database**
   - Replaced Chroma for production use
   - HNSW indexing (3x faster than Chroma)
   - Persistent storage (no data loss on restart)
   - Quantization support for memory efficiency

3. **5-Layer Caching System**

```python
# L1: Query Results (Redis)
cache_key = sha256(query + context + model + temp)
if cached := redis.get(cache_key):
    return cached  # <1ms

# L2: Embeddings (In-memory LRU)
@lru_cache(maxsize=10000)
def get_embedding(text: str):
    return embedding_model.encode(text)  # <0.1ms if cached

# L3: Vector Search Results (Qdrant internal)
# Qdrant caches hot segments automatically

# L4: LLM Outputs (Redis, TTL=1hr)
# Same key as L1

# L5: Document Chunks (Qdrant persistent)
# HNSW index with memory mapping
```

**Architecture Decision #3**: Why 5 cache layers?

**Reasoning**: Different data types need different caching strategies.

| Layer | Data Type | Storage | TTL | Hit Rate |
|-------|-----------|---------|-----|----------|
| L1 | Full query results | Redis | 1 hour | 60-85% |
| L2 | Text embeddings | Memory (LRU) | Session | 40-60% |
| L3 | Vector results | Qdrant internal | Dynamic | 70-90% |
| L4 | LLM responses | Redis | 1 hour | Same as L1 |
| L5 | Document chunks | Qdrant disk | Permanent | 100% |

**Impact**: Cached queries went from 12s → <1ms (99.9% reduction).

---

4. **Real-Time Streaming**

```typescript
// Frontend: Server-Sent Events
const eventSource = new EventSource('/api/query/stream');

eventSource.onmessage = (event) => {
  const token = JSON.parse(event.data).token;
  appendToMessage(token);  // Update UI incrementally
};
```

**Impact**: Perceived latency dropped from 12s → 500ms (time to first token).

---

**Performance**:
- Query time: 12-15 seconds (uncached), <1ms (cached)
- Accuracy: 83% (+2% vs v3.0)
- Memory: 180MB desktop + 16GB backend

**Remaining bottleneck**: Ollama HTTP overhead (200-300ms per request).

---

### v4.1 - GPU Acceleration (October 2024)

**Goal**: Eliminate HTTP overhead through direct GPU integration.

```
Tauri Desktop Shell
         │
         │ HTTP/REST
         ▼
┌─────────────────────────────────────┐
│     FastAPI Backend                 │
│                                     │
│  ┌───────────────────────────────┐ │
│  │  llama-cpp-python + CUDA      │ │
│  │  (Direct GPU access)          │ │
│  │                               │ │
│  │  ┌─────────────────────────┐ │ │
│  │  │ Single-threaded executor│ │ │
│  │  │ (thread safety)         │ │ │
│  │  └─────────────────────────┘ │ │
│  │                               │ │
│  │  Model A: Qwen 2.5 14B (Q8)  │ │
│  │  Model B: Llama 3.1 8B (Q5)  │ │
│  │  Hotswap: 3 seconds          │ │
│  └───────────────────────────────┘ │
│                                     │
│  ┌───────────────────────────────┐ │
│  │  RAG Pipeline (optimized)     │ │
│  │  - KV cache preserved ✓       │ │
│  │  - Full GPU offload ✓         │ │
│  │  - Batch reranking ✓          │ │
│  │  - Smart truncation ✓         │ │
│  │  - Parallel confidence ✓      │ │
│  └───────────────────────────────┘ │
└─────────────────────────────────────┘
```

**Critical Architectural Changes**:

1. **Ollama → llama.cpp Migration**

**Why the change was needed**:
```
Ollama Architecture:
  Request → HTTP → Ollama Server → llama.cpp → CUDA → GPU
  Overhead: 200-300ms per request (HTTP + JSON serialization)

llama-cpp-python Architecture:
  Request → llama.cpp Python bindings → CUDA → GPU
  Overhead: <1ms (direct memory access)

Speedup: 200-300ms saved per query
```

**Implementation**:
```python
# BEFORE (Ollama)
import requests
response = requests.post('http://localhost:11434/api/generate', json={
    'model': 'llama3.1:8b',
    'prompt': prompt
})

# AFTER (llama.cpp)
from llama_cpp import Llama

llm = Llama(
    model_path="models/llama-3.1-8b-q5.gguf",
    n_gpu_layers=40,  # Full GPU offload
    n_ctx=8192,       # Context window
    n_batch=512,      # Batch size
)

response = llm(prompt)  # Direct GPU inference
```

**Challenge**: Thread safety with asyncio.

**Solution**: Single-threaded executor.
```python
import concurrent.futures

# Create single-threaded executor
executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)

# All LLM calls go through executor
async def generate(prompt: str):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, llm, prompt)
```

**Why single-threaded**: llama.cpp isn't thread-safe. Concurrent calls would cause crashes.

---

2. **Runtime Model Hotswapping**

**The problem**: Users wanted both speed (Llama 8B, 90 tok/s) and quality (Qwen 14B, 25 tok/s).

**The solution**: Load/unload models on demand.

```python
class ModelManager:
    def __init__(self):
        self.current_model = None
        self.model_configs = {
            "qwen-14b-q8": {
                "path": "models/qwen2.5-14b-instruct-q8_0.gguf",
                "n_gpu_layers": 40,
                "expected_vram": 14.0,
                "expected_tok_s": 25.0
            },
            "llama-8b-q5": {
                "path": "models/llama-3.1-8b-q5.gguf",
                "n_gpu_layers": 40,
                "expected_vram": 6.0,
                "expected_tok_s": 90.0
            }
        }

    def switch_model(self, model_id: str):
        # Unload current model (releases VRAM)
        if self.current_model:
            del self.current_model
            gc.collect()  # Force garbage collection

        # Load new model (3-5 seconds)
        config = self.model_configs[model_id]
        self.current_model = Llama(
            model_path=config["path"],
            n_gpu_layers=config["n_gpu_layers"]
        )

        # Clear query cache (new model = new responses)
        redis_client.flushdb()

        return f"Switched to {model_id}"
```

**Downtime**: 3-5 seconds during switch (acceptable for user-initiated change).

---

3. **KV Cache Preservation**

**The controversial optimization**: Don't clear KV cache between queries.

**Traditional approach**:
```python
def generate(prompt):
    self.llm.reset()  # Clear KV cache (10-15s)
    return self.llm(prompt)
```

**Apollo approach**:
```python
def generate(prompt):
    # NO cache clearing - preserve warm cache
    return self.llm(prompt)
```

**Trade-off**:
- **Gain**: 40-60% speedup (10-15s saved)
- **Cost**: Minor context contamination between queries

**Quality testing**: Improved accuracy by 2% (context continuity helped).

**Decision**: Accepted. For policy Q&A, context bleed is minimal and outweighed by speedup.

---

**Performance**:
- Query time: 2-3 seconds (uncached), <1ms (cached)
- Accuracy: 85% (+2% vs v4.0)
- Inference speed: 63.8 tok/s (GPU), 4.1 tok/s (CPU)
- Speedup: 15.6x (GPU vs CPU), 7-12x (end-to-end vs v4.0)

**Breakthrough**: Production-ready performance with consumer hardware.

---

### v4.2 - Production Hardening (October 2024)

**Goal**: Operational excellence for production deployment.

**Changes** (no architecture modifications):

1. **Error Handling**
   - Graceful degradation (fallback to cache)
   - Timeout handling (120s max per query)
   - Automatic retry with exponential backoff

2. **Monitoring**
   - Health check endpoints (`/api/health`)
   - Performance metrics (`/api/metrics`)
   - Cache statistics dashboard

3. **Documentation**
   - User guides (installation, usage, troubleshooting)
   - API documentation (OpenAPI spec)
   - Architecture diagrams (you're reading them)

4. **Deployment**
   - Docker Compose (one-command startup)
   - Cross-platform builds (Windows, macOS, Linux)
   - Auto-update support (Tauri updater)

**Performance**: Same as v4.1 (2.2s average, <1ms cached)
**Quality**: Same as v4.1 (85% accuracy)
**Readiness**: **PRODUCTION-READY**

---

## Key Architectural Decisions & Trade-offs

### Decision 1: Tauri vs Electron

**Options**:
- Electron (JavaScript runtime, 200MB+ footprint)
- Tauri (Rust + OS webview, 50MB footprint)

**Choice**: Tauri

**Reasoning**:
| Factor | Electron | Tauri |
|--------|----------|-------|
| Memory | 200-300MB | 50MB |
| Security | Node.js access | Rust sandboxing |
| Build size | 150-200MB | 10-15MB |
| Startup time | 2-3s | <1s |

**Trade-off**: Tauri has steeper learning curve (Rust vs JavaScript). Accepted for 4x smaller footprint.

---

### Decision 2: Qdrant vs Chroma

**Options**:
- Chroma (Python-native, easy setup)
- Qdrant (Rust-based, production-grade)

**Choice**: Qdrant

**Reasoning**:
| Feature | Chroma | Qdrant |
|---------|--------|--------|
| Query speed | 600ms | 200ms |
| Persistence | SQLite | Custom engine |
| Quantization | No | Yes (4x smaller) |
| Production use | Limited | Battle-tested |

**Trade-off**: Qdrant requires Docker. Accepted for 3x faster queries.

---

### Decision 3: llama.cpp vs vLLM

**Options**:
- llama.cpp (CPU/GPU, GGUF models, single requests)
- vLLM (GPU-only, continuous batching, high throughput)

**Choice**: llama.cpp

**Reasoning**:
| Factor | llama.cpp | vLLM |
|--------|-----------|------|
| Latency (single) | 2-3s | 3-5s |
| Throughput (multi) | 1 query/3s | 10 queries/3s |
| CPU support | Yes | No |
| Complexity | Low | High |

**Trade-off**: vLLM better for multi-user throughput. llama.cpp better for single-user latency. Apollo targets single-user desktop, so llama.cpp wins.

---

### Decision 4: Rerank Preset System

**Options**:
- Fixed top_n (always retrieve 5 documents)
- User-controlled presets (Quick/Quality/Deep)

**Choice**: User-controlled presets

**Reasoning**: Different queries need different depth.
- Simple: "What is the max beard length?" → 2 docs sufficient
- Complex: "Compare officer vs enlisted policies" → 5 docs needed

**Implementation**:
```typescript
interface Settings {
  rerankPreset: 'quick' | 'quality' | 'deep';
}

const PRESET_CONFIG = {
  quick: { top_n: 2, rerank_docs: 2 },    // 800ms
  quality: { top_n: 3, rerank_docs: 3 },  // 1200ms
  deep: { top_n: 5, rerank_docs: 5 }      // 2000ms
};
```

**Trade-off**: Adds UI complexity. Accepted for 3-5x speedup on simple queries.

---

## Architecture Patterns & Principles

### Pattern 1: Defense in Depth (Caching)

**Principle**: Multiple cache layers with different characteristics.

```
Request arrives
  ↓
L1: Query result cache (Redis, <1ms)
  ↓ Cache miss
L2: Embedding cache (Memory, <0.1ms)
  ↓ Cache miss
L3: Vector search cache (Qdrant, <5ms)
  ↓ Cache miss
L4: LLM output cache (Redis, <1ms)
  ↓ Cache miss
L5: Document storage (Qdrant, always hits)
  ↓
Generate fresh response (2-3s)
```

**Effectiveness**: 99.5% latency reduction for cached queries.

---

### Pattern 2: Graceful Degradation

**Principle**: System remains functional even when components fail.

**Example**: Vector DB unavailable
```python
try:
    results = await qdrant.search(query_vector, top_k=10)
except QdrantException:
    logger.warning("Qdrant unavailable, using cache")
    results = await redis.get(f"fallback:{query_hash}")
    if not results:
        return {"error": "Vector DB unavailable, try later"}
```

**Fallback chain**:
1. Primary: Qdrant vector search
2. Fallback 1: Redis cached results
3. Fallback 2: Error message with retry suggestion

---

### Pattern 3: Single Responsibility Separation

**Principle**: Each component does one thing well.

**Example**: Retrieval pipeline
```python
# Embedding: Only convert text → vectors
class EmbeddingService:
    def encode(self, text: str) -> np.ndarray

# VectorDB: Only store/retrieve vectors
class VectorStore:
    def search(self, vector: np.ndarray) -> List[Document]

# Reranker: Only score documents
class Reranker:
    def rerank(self, query: str, docs: List[Document]) -> List[Document]

# Pipeline: Orchestrate components
class RAGPipeline:
    def query(self, text: str):
        vector = embedding.encode(text)
        docs = vector_store.search(vector)
        ranked = reranker.rerank(text, docs)
        return ranked
```

**Benefit**: Easy to swap components (e.g., Chroma → Qdrant without touching reranker).

---

### Pattern 4: Instrumentation by Default

**Principle**: Every operation logs timing and metadata.

```python
class InstrumentedRAGPipeline:
    def __init__(self):
        self.timer = TimingLogger()

    async def query(self, text: str):
        # Log every stage
        with self.timer.measure("embedding"):
            vector = await self.embedding.encode(text)

        with self.timer.measure("vector_search"):
            docs = await self.vector_db.search(vector)

        # ... more stages

        # Return result + timing breakdown
        return {
            "answer": answer,
            "timing_breakdown": self.timer.get_breakdown()
        }
```

**Impact**: Performance regression detection in production. Any stage >200ms triggers alert.

---

## Lessons From Architecture Evolution

### Lesson 1: Start Simple, Add Complexity Only When Needed

**v1.0**: Single vector search, no reranking, no caching
**v2.0**: Added hybrid search (complexity +20%, quality +12%)
**v3.0**: Added reranking (complexity +30%, quality +9%)
**v4.0**: Added caching (complexity +15%, speed +10x)

**Principle**: Each added complexity must justify itself with measurable improvement.

---

### Lesson 2: Measure Before Migrating

**Bad**: "Let's switch to Qdrant, it's faster"
**Good**: "Chroma takes 600ms per query. Qdrant benchmarks show 200ms. Let's test with our data."

**Testing approach**:
1. Benchmark current system (Chroma: 600ms avg)
2. Set up Qdrant with same data
3. Benchmark Qdrant (210ms avg)
4. Validate quality maintained (83% accuracy both)
5. Migrate

**Result**: 3x speedup confirmed before production migration.

---

### Lesson 3: Architecture Serves Users, Not Developers

**Developer preference**: "I want to use cutting-edge Graph RAG"
**User need**: "I want answers in under 3 seconds"

**Reality check**:
- Graph RAG: 5-10s queries, 2-3x quality for complex questions
- Current system: 2-3s queries, 85% accuracy

**Decision**: Ship current system. Explore Graph RAG for v5.0 "Pro" tier.

**Principle**: Ship what users need, not what's technically interesting.

---

### Lesson 4: Backward Compatibility Enables Iteration

**Every version maintained API compatibility**:
```python
# v1.0 API
POST /api/query
{
  "query": "What is the max beard length?"
}

# v4.2 API (same interface, more options)
POST /api/query
{
  "query": "What is the max beard length?",
  "rerank_preset": "quality",  # Optional
  "use_cache": true            # Optional
}
```

**Benefit**: Frontend never broke. Backend could evolve independently.

---

## The Current Architecture (v4.2)

### Component Diagram

```
┌─────────────────────────────────────────────────────────┐
│              User's Desktop (Windows/macOS/Linux)       │
│                                                         │
│  ┌───────────────────────────────────────────────────┐ │
│  │           Tauri Shell (Rust 1.77+)                │ │
│  │  - Native OS integration                          │ │
│  │  - File system access (capability-based)          │ │
│  │  - IPC with React frontend                        │ │
│  └─────────────────────┬─────────────────────────────┘ │
│                        │                                │
│  ┌─────────────────────▼─────────────────────────────┐ │
│  │      React 19 Frontend (TypeScript)               │ │
│  │  - SSE streaming (real-time tokens)               │ │
│  │  - Zustand state management                       │ │
│  │  - Performance dashboard                          │ │
│  └─────────────────────┬─────────────────────────────┘ │
└────────────────────────┼───────────────────────────────┘
                         │ HTTP/REST + SSE
            ┌────────────┴────────────┐
            │                         │
      ┌─────▼────────┐         ┌──────▼──────────────────┐
      │ Redis 7.2    │         │  FastAPI Backend        │
      │              │         │  (Python 3.11+)         │
      │ - L1 cache   │         │                         │
      │ - L4 cache   │         │ ┌─────────────────────┐ │
      │ - TTL=1hr    │         │ │ llama-cpp-python    │ │
      └──────────────┘         │ │ + CUDA 12.1         │ │
                               │ │ - 63.8 tok/s GPU    │ │
                               │ │ - KV cache preserve │ │
                               │ │ - Model hotswap     │ │
                               │ └─────────────────────┘ │
                               │                         │
                               │ ┌─────────────────────┐ │
                               │ │ RAG Pipeline        │ │
                               │ │ - Hybrid retrieval  │ │
                               │ │ - 2-stage reranking │ │
                               │ │ - Smart truncation  │ │
                               │ │ - Parallel scoring  │ │
                               │ └─────────────────────┘ │
                               │                         │
                               │ ┌─────────────────────┐ │
                               │ │ Embedding Service   │ │
                               │ │ BGE-large-en-v1.5   │ │
                               │ │ (1024-dim)          │ │
                               │ └─────────────────────┘ │
                               └───────┬─────────────────┘
                                       │
                                ┌──────▼──────────┐
                                │  Qdrant 1.8.0   │
                                │  - HNSW indexing│
                                │  - <5ms queries │
                                │  - Quantization │
                                └─────────────────┘
```

### Data Flow

```
User submits query → Tauri IPC → React frontend
  ↓
React → POST /api/query → FastAPI backend
  ↓
Check L1 cache (Redis)
  ↓ Cache miss
Generate embeddings → Check L2 cache (LRU)
  ↓ Cache miss
Vector search (Qdrant) + BM25 search → Check L3 cache (Qdrant)
  ↓ Fresh results
RRF fusion → Cross-encoder rerank → LLM rerank
  ↓
Generate answer with llama.cpp (GPU-accelerated)
  ↓ Parallel
Calculate confidence (retrieval + answer quality)
  ↓
Store in L1 cache (Redis) → Stream tokens via SSE → React
  ↓
User sees answer incrementally (500ms to first token)
```

---

## Future Architecture Vision

### v5.0 - Advanced Retrieval (Q2 2025)

**Planned additions**:
- BGE Reranker v2-m3 (replace LLM reranking, 85% faster)
- Speculative decoding (1.5-2x generation speedup)
- ColBERT late interaction (token-level matching, +20-40% quality)

**Architecture impact**: Minimal. Drop-in replacements for existing components.

---

### v6.0 - Multi-Modal & Agentic (Q3 2025)

**Planned additions**:
- Graph RAG (knowledge graph for multi-hop reasoning)
- ColPali (visual document retrieval for charts/diagrams)
- Agentic RAG (query planning, tool use, self-correction)

**Architecture impact**: Significant. New components with complex orchestration.

**Design challenge**: Maintain <3s latency for simple queries while enabling deep research mode (10-30s) for complex queries.

---

## Conclusion: Architecture as Evolution

**Apollo's architecture didn't emerge fully formed.** It evolved through:

1. **Iterative improvement** (v1.0 → v4.2 over 10 months)
2. **Measured decision-making** (every change benchmarked)
3. **User-centric design** (architecture serves needs, not ego)
4. **Backward compatibility** (no breaking changes across versions)

**The result**: A production-ready system that's 12x faster, 25% more accurate, and fully documented.

**The lesson**: Good architecture is the result of systematic improvement, not grand design.

---

<div className="architecture-footer">

### Explore Further

[Read the Performance Story →](/journey/performance-optimization)
[Return to Journey Overview →](/journey/overview)
[View Full Documentation →](/architecture)

</div>

---

<div className="technical-specs">
**Current Stack (v4.2)**:
Desktop: Tauri 2.9, Rust 1.77+, React 19, TypeScript 5.6
Backend: Python 3.11, FastAPI, llama-cpp-python + CUDA 12.1
Storage: Qdrant 1.8.0, Redis 7.2
Models: Qwen 2.5 14B (Q8), Llama 3.1 8B (Q5), BGE-large-en-v1.5

**Performance (v4.2)**:
Query time: 2.2s avg (P50), <1ms cached
Accuracy: 85%
Inference: 63.8 tok/s (GPU)
Memory: 50MB desktop, 16GB backend
</div>
