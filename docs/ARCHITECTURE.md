# Tactical RAG System - Architecture Documentation

**Version**: 3.5.1
**Last Updated**: 2025-10-11
**Created By**: MEDICANT_BIAS / Enhanced by: HOLLOWED_EYES + ZHADYZ

---

## System Overview

The Tactical RAG System is an enterprise-grade document intelligence platform that employs adaptive retrieval strategies, GPU-accelerated processing, and real-time performance monitoring to deliver accurate, context-aware answers from document repositories.

### Core Design Principles

1. **Adaptive Intelligence**: Query complexity determines retrieval strategy automatically
2. **Conversation Memory**: Multi-turn context tracking for natural follow-up questions ðŸ†•
3. **Transparent Explainability**: Full reasoning visibility for all AI decisions ðŸ†•
4. **GPU Acceleration**: All compute-intensive operations leverage CUDA when available
5. **Multi-Layer Caching**: Aggressive caching at embedding, query, and result levels
6. **Production-Ready**: Comprehensive monitoring, evaluation, and error handling
7. **Docker-Native**: Full containerization with GPU passthrough support

---

## Architecture Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRESENTATION LAYER                        â”‚
â”‚  â€¢ Gradio Web Interface (web_interface.py)                  â”‚
â”‚  â€¢ Real-time Performance Dashboard                           â”‚
â”‚  â€¢ Dynamic Settings Panel                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APPLICATION LAYER                         â”‚
â”‚  â€¢ Main Orchestrator (app.py)                               â”‚
â”‚  â€¢ Query Processing Pipeline                                 â”‚
â”‚  â€¢ Conversation Memory (conversation_memory.py) ðŸ†•          â”‚
â”‚  â€¢ Explainability System (explainability.py) ðŸ†•              â”‚
â”‚  â€¢ Context Management                                        â”‚
â”‚  â€¢ Dynamic Settings Management                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INTELLIGENCE LAYER                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Adaptive Retrieval Engine                    â”‚  â”‚
â”‚  â”‚         (adaptive_retrieval.py)                      â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  Query Classifier                              â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Multi-factor scoring                        â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Length, type, complexity analysis           â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Dynamic threshold adjustment                â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚   SIMPLE     â”‚   HYBRID     â”‚    ADVANCED      â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   STRATEGY   â”‚   STRATEGY   â”‚    STRATEGY      â”‚  â”‚  â”‚
â”‚  â”‚  â”‚              â”‚              â”‚                  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Vector     â”‚ â€¢ Dense      â”‚ â€¢ Multi-query   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   similarity â”‚   (vector)   â”‚   expansion     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Top-K      â”‚ â€¢ Sparse     â”‚ â€¢ LLM variants  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Direct     â”‚   (BM25)     â”‚ â€¢ Vote          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   return     â”‚ â€¢ RRF fusion â”‚   aggregation   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚              â”‚ â€¢ Cross-     â”‚ â€¢ Cross-        â”‚  â”‚  â”‚
â”‚  â”‚  â”‚              â”‚   encoder    â”‚   encoder       â”‚  â”‚  â”‚
â”‚  â”‚  â”‚              â”‚   reranking  â”‚   reranking     â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Answer Generator                                     â”‚  â”‚
â”‚  â”‚  â€¢ Adaptive prompting based on query type            â”‚  â”‚
â”‚  â”‚  â€¢ Source citation enforcement                       â”‚  â”‚
â”‚  â”‚  â€¢ Hallucination prevention                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA ACCESS LAYER                         â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  Vector Store    â”‚  â”‚  Sparse Index    â”‚                â”‚
â”‚  â”‚  (ChromaDB)      â”‚  â”‚  (BM25)          â”‚                â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚                â”‚
â”‚  â”‚ â€¢ 768-dim        â”‚  â”‚ â€¢ Term frequency â”‚                â”‚
â”‚  â”‚   embeddings     â”‚  â”‚ â€¢ Keyword match  â”‚                â”‚
â”‚  â”‚ â€¢ HNSW index     â”‚  â”‚ â€¢ Fast lookup    â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Multi-Layer Cache Manager                    â”‚  â”‚
â”‚  â”‚         (cache_and_monitoring.py)                    â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  â€¢ Embedding Cache (10K entries, 1hr TTL)            â”‚  â”‚
â”‚  â”‚  â€¢ Query Cache (1K entries, 1hr TTL)                 â”‚  â”‚
â”‚  â”‚  â€¢ Result Cache (2K entries, 1hr TTL)                â”‚  â”‚
â”‚  â”‚  â€¢ Thread-safe LRU with TTL enforcement              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCUMENT PROCESSING LAYER                 â”‚
â”‚  (document_processor.py)                                    â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Document Loader (Parallel Processing)               â”‚  â”‚
â”‚  â”‚  â€¢ PDF (PyPDF + Tesseract OCR fallback)              â”‚  â”‚
â”‚  â”‚  â€¢ DOCX/DOC (Docx2txt)                               â”‚  â”‚
â”‚  â”‚  â€¢ TXT (Multi-encoding detection)                    â”‚  â”‚
â”‚  â”‚  â€¢ Markdown (Structured parsing)                     â”‚  â”‚
â”‚  â”‚  â€¢ ThreadPoolExecutor (4 workers)                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Intelligent Chunker                                  â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Strategies:                                          â”‚  â”‚
â”‚  â”‚  â€¢ Recursive: Hierarchical splitting (default)       â”‚  â”‚
â”‚  â”‚  â€¢ Semantic: Embedding-based boundaries (GPU)        â”‚  â”‚
â”‚  â”‚  â€¢ Sentence: Fixed sentence count                    â”‚  â”‚
â”‚  â”‚  â€¢ Hybrid: Recursive + semantic refinement (best)    â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Metadata Enrichment:                                 â”‚  â”‚
â”‚  â”‚  â€¢ File metadata (name, type, hash, size)            â”‚  â”‚
â”‚  â”‚  â€¢ Chunk position (index, page, total)               â”‚  â”‚
â”‚  â”‚  â€¢ Processing metadata (strategy, timestamp)         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI MODELS LAYER (Ollama)                  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LLM: llama3.1:8b                                     â”‚  â”‚
â”‚  â”‚  â€¢ 8B parameters                                      â”‚  â”‚
â”‚  â”‚  â€¢ 4096 token context window (optimized)             â”‚  â”‚
â”‚  â”‚  â€¢ Temperature: 0.0 (deterministic)                  â”‚  â”‚
â”‚  â”‚  â€¢ Full GPU layer loading                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Embeddings: nomic-embed-text                        â”‚  â”‚
â”‚  â”‚  â€¢ 768-dimensional vectors                           â”‚  â”‚
â”‚  â”‚  â€¢ 8192 token context                                â”‚  â”‚
â”‚  â”‚  â€¢ GPU-accelerated via Ollama                        â”‚  â”‚
â”‚  â”‚  â€¢ Batch size: 32                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2     â”‚  â”‚
â”‚  â”‚  â€¢ Cross-attention scoring                           â”‚  â”‚
â”‚  â”‚  â€¢ GPU-accelerated (CUDA)                            â”‚  â”‚
â”‚  â”‚  â€¢ Batch processing (16 pairs/batch)                 â”‚  â”‚
â”‚  â”‚  â€¢ ~10x faster than CPU                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Details

### 1. Adaptive Retrieval Engine

**Purpose**: Automatically select optimal retrieval strategy based on query complexity.

**Algorithm**: Multi-factor query classification
- **Length scoring**: Word count â†’ complexity correlation
- **Question type detection**: Who/what/why patterns
- **Complexity indicators**: Boolean operators, multiple questions

**Strategies**:

1. **Simple (score â‰¤ 1)**
   - Input: Factual queries ("Who is X?")
   - Method: Pure vector similarity search
   - Output: Top 3 chunks, normalized scores

2. **Hybrid (score â‰¤ 3)**
   - Input: Moderate queries ("What are the requirements?")
   - Method: RRF fusion of dense + sparse retrieval
   - Process:
     1. Dense retrieval (vector similarity, K=20)
     2. Sparse retrieval (BM25, K=20)
     3. RRF score calculation: `score = Î£(1 / (k + rank))`
     4. Cross-encoder reranking on GPU
     5. Weighted fusion: `final = 0.3*RRF + 0.7*rerank`
   - Output: Top 5 reranked chunks

3. **Advanced (score > 3)**
   - Input: Complex queries ("Compare X and Y")
   - Method: Multi-query expansion with consensus
   - Process:
     1. LLM generates 2 query variants
     2. Search with original + variants (K=15 each)
     3. Vote aggregation (chunks in multiple results ranked higher)
     4. Cross-encoder reranking on GPU
   - Output: Top 5 consensus chunks

### 2. Conversation Memory System ðŸ†•

**Purpose**: Maintain multi-turn conversation context for natural follow-up questions.

**Architecture**:
- **Sliding Window**: FIFO queue storing last 10 exchanges (deque with maxlen)
- **Exchange Storage**: ConversationExchange dataclass with query, response, retrieved_docs, query_type, strategy_used, timestamp
- **Automatic Summarization**: LLM-based compression triggered every 5 exchanges
- **Thread Safety**: RLock for concurrent access

**Key Features**:

1. **Follow-Up Detection**
   - Pattern matching: "that", "those", "this", "tell me more", "what about"
   - Short query heuristic: <10 words + history = likely follow-up
   - Returns: boolean indicating if current query references previous context

2. **Context Enhancement**
   - Method: `get_relevant_context_for_query(query, max_exchanges=5)`
   - Returns: (enhanced_query, relevant_documents)
   - Enhanced query includes conversation summary + recent exchanges
   - Relevant documents collected from previous turns

3. **Automatic Summarization**
   - Trigger: Every 5 exchanges (configurable)
   - Method: LLM generates <200 word summary preserving key topics
   - Compression: Typical 70-80% reduction in context size
   - Cumulative: New summaries append to existing summary

4. **Statistics Tracking**
   - Total exchanges, current window size, summarizations performed
   - Memory usage tracking
   - Hit/miss metrics for follow-up detection

**Integration Points**:
- app.py query() method: Automatic context enhancement for follow-ups
- app.py clear_conversation(): Manual memory reset
- Web interface: "Clear Chat" button triggers memory reset

**Performance Characteristics**:
- Follow-up detection: <10ms
- Context retrieval: <50ms
- Summarization: ~1-2 seconds (async, non-blocking)
- Memory per exchange: ~2KB
- Max memory (10 exchanges + summary): ~25KB

### 3. Explainability System ðŸ†•

**Purpose**: Provide transparent reasoning for query classification and retrieval strategy selection.

**Architecture**:
- **QueryExplanation Dataclass**: Structured explanation object with all decision factors
- **Factory Function**: `create_query_explanation()` auto-generates explanations
- **Integration**: Explanation objects flow through retrieval pipeline transparently

**Key Components**:

1. **Explanation Data Structure**
   ```python
   @dataclass
   class QueryExplanation:
       query_type: str              # Classification result
       complexity_score: int        # Total score from multi-factor analysis
       scoring_breakdown: Dict      # Factor â†’ contribution mapping
       thresholds_used: Dict        # Classification threshold values
       strategy_selected: str       # Retrieval strategy chosen
       strategy_reasoning: str      # Human-readable rationale
       key_factors: List[str]       # Primary contributing factors
       example_text: str            # Auto-generated explanation
   ```

2. **Explanation Generation**
   - **Scoring Breakdown**: Captures each classification factor's contribution
     - Length scoring: "12 words (+2)"
     - Question type: "why (+3)"
     - Complexity indicators: "has_and_operator=yes (+1)"

   - **Strategy Mapping**: Automatic strategy selection reasoning
     - simple â†’ "Straightforward query requires only dense vector retrieval"
     - moderate â†’ "Moderate complexity benefits from hybrid BM25+dense with reranking"
     - complex â†’ "High complexity requires query expansion and advanced fusion"

   - **Key Factors Extraction**: Identifies factors that contributed points (+ values)

   - **Human-Readable Text**: Auto-generated explanation combining all factors
     ```
     Query classified as COMPLEX (score: 5) because: length=12 words (+2),
     question_type=why (+3), has_and_operator=yes (+1). Thresholds: simpleâ‰¤1,
     moderateâ‰¤3. Using advanced_expanded strategy. Reasoning: High complexity
     requires query expansion and advanced fusion
     ```

3. **Serialization Support**
   - `to_dict()`: Convert to dictionary for JSON storage/logging
   - `from_dict()`: Restore from dictionary
   - Enables audit logging, debugging, and analytics

**Integration Points**:
- `adaptive_retrieval.py _classify_query()`: Generates explanation during classification
- `RetrievalResult`: Includes explanation object
- `app.py query()`: Logs explanation for audit trail
- `web_interface.py`: Optional UI display (collapsible section)

**Use Cases**:
- **User Transparency**: Show users why they got specific results
- **Developer Debugging**: Diagnose classification issues, tune thresholds
- **Audit Compliance**: Log AI decision reasoning for government/enterprise requirements
- **System Optimization**: Analyze explanation patterns to improve classification

**Performance Characteristics**:
- Explanation generation: <1ms (negligible overhead)
- Memory per explanation: ~500 bytes
- Serialization: <1ms for JSON conversion
- Zero impact on query latency

**Testing**:
- Unit tests: `test_explainability.py` (12 test cases)
- Covers: initialization, serialization, factory function, text generation
- Edge cases: empty breakdowns, missing fields, round-trip conversion

### 4. Document Processing Pipeline

**Parallel Loading**:
- ThreadPoolExecutor with 4 workers
- Per-document error isolation
- Multi-format support (PDF, DOCX, TXT, MD)
- OCR fallback for scanned PDFs

**Chunking Strategies**:

1. **Recursive** (Default): Hierarchical splitting with semantic separators
2. **Semantic**: Sentence embeddings + boundary detection (GPU-accelerated)
3. **Hybrid**: Recursive â†’ semantic refinement for large chunks (Best balance)

**Metadata Enrichment**:
- File identification: name, type, hash, size
- Position tracking: chunk_index, page_number, total_chunks
- Processing info: strategy used, timestamp

### 3. Caching Infrastructure

**Three-Layer LRU Cache**:

1. **Embedding Cache** (10K entries, 1hr TTL)
   - Purpose: Cache expensive embedding computations
   - Key: MD5(text)
   - Benefit: ~90% hit rate on repeated content

2. **Query Cache** (1K entries, 1hr TTL)
   - Purpose: Cache complete query results
   - Key: MD5(query + parameters)
   - Benefit: Instant responses for duplicate queries

3. **Result Cache** (2K entries, 1hr TTL)
   - Purpose: Cache retrieval results before generation
   - Key: MD5(query)
   - Benefit: Fast re-generation with different prompts

**Features**:
- Thread-safe with RLock
- Automatic TTL expiration
- LRU eviction when full
- Hit/miss/eviction statistics

### 4. Performance Monitoring

**PyTorch-Based GPU Monitoring** (Docker-compatible):
- GPU utilization (estimated from memory reservation)
- VRAM usage (allocated + reserved in MB)
- CPU utilization (psutil)
- No nvidia-smi dependency

**Metrics Collection**:
- Query count & latency (avg, p50, p95, p99)
- Retrieval stage timing
- Cache hit rates
- Error rates
- Real-time updates (1 second interval)

### 5. Evaluation Framework

**Automated Testing** (evaluate.py):

1. **Retrieval Quality**
   - Precision, Recall, F1 score
   - Mean Reciprocal Rank (MRR)
   - By-difficulty breakdown

2. **Answer Quality**
   - Answer length statistics
   - Source citation count
   - Query type distribution

3. **Performance Metrics**
   - Cold/warm latency comparison
   - Cache effectiveness measurement
   - Percentile analysis

4. **Stress Testing**
   - Concurrent query handling
   - Throughput measurement (QPS)
   - Error rate under load

**Grading System**: A (90-100) â†’ F (<60) with automated recommendations

---

## Technology Stack

### Core Framework
- **Language**: Python 3.11
- **Web UI**: Gradio (chat interface + monitoring dashboard)
- **Async**: asyncio (concurrent operations)
- **Parallelism**: ThreadPoolExecutor (document loading)

### AI/ML Stack
- **LLM Platform**: Ollama (local inference)
- **Models**:
  - LLM: llama3.1:8b (8B parameter model)
  - Embeddings: nomic-embed-text (768-dim)
  - Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2
- **GPU**: CUDA 12.1+ (PyTorch backend)
- **Frameworks**: LangChain, sentence-transformers

### Data Layer
- **Vector DB**: ChromaDB (HNSW index)
- **Sparse Index**: BM25 (keyword matching)
- **Caching**: In-memory LRU caches

### Document Processing
- **PDF**: PyPDF + Tesseract OCR (fallback)
- **DOCX**: Docx2txt
- **Text**: Multi-encoding detection
- **Markdown**: Unstructured

### Infrastructure
- **Containerization**: Docker + Docker Compose
- **GPU Passthrough**: NVIDIA Container Toolkit
- **Networking**: Bridge network for inter-service communication
- **Volumes**: Persistent storage for documents, database, logs

---

## Integration Points

### Internal Communication
1. **App â†” Adaptive Retriever**: Query submission, retrieval results
2. **Adaptive Retriever â†” Data Layer**: Vector search, BM25 search
3. **App â†” Cache Manager**: Query/result caching
4. **App â†” Performance Monitor**: Metrics collection
5. **Web Interface â†” App**: User queries, status updates

### External Dependencies
1. **Ollama API** (port 11434): LLM inference, embeddings
2. **ChromaDB**: Vector storage and similarity search
3. **Docker Network**: Service-to-service communication

---

## Deployment Architecture

```
Docker Compose Services:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama Service    â”‚
â”‚   Port: 11434       â”‚
â”‚   GPU: NVIDIA       â”‚
â”‚   Models:           â”‚
â”‚   - llama3.1:8b     â”‚
â”‚   - nomic-embed-textâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    Docker Bridge Network
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RAG App Service   â”‚
â”‚   Port: 7860        â”‚
â”‚   GPU: NVIDIA       â”‚
â”‚   Runtime: nvidia   â”‚
â”‚   Volumes:          â”‚
â”‚   - ./documents     â”‚
â”‚   - ./chroma_db     â”‚
â”‚   - ./logs          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features**:
- Health checks on Ollama before RAG app starts
- GPU passthrough to both services
- Persistent volumes for data
- Automatic restart on failure

---

## Data Flow

### Query Processing Flow

```
1. User submits query via Gradio
   â†“
2. App.py receives query
   â†“
3. Check query cache â†’ [HIT] Return cached result â†’ END
   â†“ [MISS]
4. AdaptiveRetriever.retrieve(query)
   â†“
5. Classify query (simple/moderate/complex)
   â†“
6. Route to appropriate strategy:
   â”œâ”€ Simple: Vector search â†’ Top 3
   â”œâ”€ Hybrid: Vector + BM25 â†’ RRF â†’ Rerank â†’ Top 5
   â””â”€ Advanced: Multi-query â†’ Vote â†’ Rerank â†’ Top 5
   â†“
7. RetrievalResult (documents + scores + metadata)
   â†“
8. AdaptiveAnswerGenerator.generate(query, result)
   â†“
9. Build context with source citations
   â†“
10. LLM generates answer with adaptive prompting
    â†“
11. Format response with sources and metadata
    â†“
12. Cache result
    â†“
13. Return to user
```

### Document Indexing Flow

```
1. Place documents in documents/ folder
   â†“
2. Run index_documents.py
   â†“
3. DocumentProcessor.process_documents()
   â†“
4. DocumentLoader loads files in parallel (ThreadPoolExecutor)
   â†“
5. For each document:
   â”œâ”€ Detect format (PDF/DOCX/TXT/MD)
   â”œâ”€ Extract text (OCR fallback for PDFs)
   â”œâ”€ Enrich metadata (hash, size, timestamp)
   â””â”€ Create Document objects
   â†“
6. IntelligentChunker.chunk(documents)
   â”œâ”€ Apply selected strategy (recursive/semantic/hybrid)
   â”œâ”€ Add chunk metadata (index, position, strategy)
   â””â”€ Validate chunks (size, content quality)
   â†“
7. Generate embeddings (Ollama, batched)
   â†“
8. Store in ChromaDB (vector database)
   â†“
9. Save metadata for BM25 (JSON file)
   â†“
10. Indexing complete â†’ System ready
```

---

## Configuration Management

**Primary Config**: `config.yml`
- Structured YAML configuration
- Environment variable overrides via `RAG_*` prefix
- Pydantic validation with type checking
- Default values for all settings

**Key Configuration Sections**:
1. **LLM Settings**: Model, temperature, context window
2. **Embedding Settings**: Model, dimension, batch size
3. **Chunking**: Strategy, size, overlap
4. **Retrieval**: K-values, fusion method, thresholds
5. **Caching**: TTL, max sizes, enable/disable flags
6. **Performance**: Workers, batching, async settings
7. **Monitoring**: Logging, metrics, tracing

---

## Security Considerations

**Current Implementation**:
- Local deployment (no external API calls)
- Docker network isolation
- Volume-based data persistence

**Future Enhancements** (Roadmap):
- Document-level access control
- Query audit logging
- PII detection in documents
- Encrypted embeddings for sensitive data
- Answer provenance tracking

---

## Performance Characteristics

**Expected Latency**:
- Simple queries: 1-2s
- Hybrid queries: 2-4s
- Advanced queries: 4-6s
- Cached queries: <100ms

**Throughput**:
- Concurrent: ~10-15 QPS (with caching)
- Cold: ~2-3 QPS

**Resource Usage**:
- CPU RAM: 4-6 GB
- VRAM: 6-8 GB (llama3.1:8b)
- Disk: ~10 GB (models + indexes)

**Optimization Strategies**:
- Reduced context window (8192â†’4096 tokens)
- Smaller chunks (800â†’500 chars)
- Fewer rerank candidates (20â†’15)
- Multi-layer caching
- GPU acceleration throughout
- Batch processing (16 docs/batch)

---

## Failure Modes & Recovery

**Document Processing Failures**:
- Per-document error isolation
- OCR fallback for unreadable PDFs
- Multi-encoding detection for text files
- Graceful degradation (skip failed files)

**Retrieval Failures**:
- Strategy fallback (advancedâ†’hybridâ†’simple)
- Empty result handling (informative messages)
- Timeout protection (async with timeouts)

**LLM Failures**:
- Retry logic (automatic)
- Fallback to cached results
- Error messages to user

**Infrastructure Failures**:
- Docker restart policies (unless-stopped)
- Health checks (Ollama service)
- Volume persistence (data survives restarts)

---

## Extension Points

**Adding New Document Formats**:
- Extend `document_processor.py`
- Register in `supported_extensions` dict
- Implement loader method

**Custom Retrieval Strategies**:
- Add to `adaptive_retrieval.py`
- Define classification rules
- Implement retrieval method

**Alternative LLMs**:
- Change model in `config.yml`
- Ensure Ollama has model pulled
- Adjust context window as needed

**Custom Chunking Strategies**:
- Extend `IntelligentChunker` class
- Implement chunk method
- Add to strategy selection

---

## Development Guidelines

**Code Quality Standards**:
- Type hints on all functions
- Comprehensive docstrings
- Error handling with logging
- Async/await for I/O operations
- Thread-safe shared state

**Testing Strategy**:
- Evaluation suite (evaluate.py)
- Manual testing via web UI
- Performance benchmarking
- Stress testing (concurrent queries)

**Version Control**:
- Git commits for each feature
- Semantic commit messages
- Feature branches for major changes

---

## Roadmap

See `state.json` for current improvement priorities and task assignments.

**Planned Enhancements**:
1. Multi-turn conversation memory
2. Explainability (query classification reasoning)
3. Feedback loops (user ratings, learning)
4. Multi-modal support (tables, figures)
5. Security hardening (audit logs, access control)

---

**Document Version**: 1.0
**Status**: Production-ready baseline
**Next Review**: After each improvement milestone
