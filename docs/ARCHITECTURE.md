# Tactical RAG System - Architecture Documentation

**Version**: 3.5.1
**Last Updated**: 2025-10-11
**Created By**: medicant_bias / Enhanced by: hollowed_eyes + zhadyz

---

## System Overview

The Tactical RAG System is an enterprise-grade document intelligence platform that employs adaptive retrieval strategies, GPU-accelerated processing, and real-time performance monitoring to deliver accurate, context-aware answers from document repositories.

### Core Design Principles

1. **Adaptive Intelligence**: Query complexity determines retrieval strategy automatically
2. **Conversation Memory**: Multi-turn context tracking for natural follow-up questions ðŸ†•
3. **Transparent Explainability**: Full reasoning visibility for all AI decisions ðŸ†•
4. **User Feedback & Continuous Improvement**: Real-time satisfaction tracking and analytics ðŸ†•
5. **GPU Acceleration**: All compute-intensive operations leverage CUDA when available
6. **Multi-Layer Caching**: Aggressive caching at embedding, query, and result levels
7. **Production-Ready**: Comprehensive monitoring, evaluation, and error handling
8. **Docker-Native**: Full containerization with GPU passthrough support

---

## Architecture Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRESENTATION LAYER                        â”‚
â”‚  â€¢ Gradio Web Interface (web_interface.py)                  â”‚
â”‚  â€¢ Real-time Performance Dashboard                           â”‚
â”‚  â€¢ Dynamic Settings Panel                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APPLICATION LAYER                         â”‚
â”‚  â€¢ Main Orchestrator (app.py)                               â”‚
â”‚  â€¢ Query Processing Pipeline                                 â”‚
â”‚  â€¢ Conversation Memory (conversation_memory.py) ðŸ†•          â”‚
â”‚  â€¢ Explainability System (explainability.py) ðŸ†•              â”‚
â”‚  â€¢ Feedback System (feedback_system.py) ðŸ†•                   â”‚
â”‚  â€¢ Context Management                                        â”‚
â”‚  â€¢ Dynamic Settings Management                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INTELLIGENCE LAYER                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Adaptive Retrieval Engine                    â”‚  â”‚
â”‚  â”‚         (adaptive_retrieval.py)                      â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  Query Classifier                              â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Multi-factor scoring                        â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Length, type, complexity analysis           â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Dynamic threshold adjustment                â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚   SIMPLE     â”‚   HYBRID     â”‚    ADVANCED      â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   STRATEGY   â”‚   STRATEGY   â”‚    STRATEGY      â”‚  â”‚  â”‚
â”‚  â”‚  â”‚              â”‚              â”‚                  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Vector     â”‚ â€¢ Dense      â”‚ â€¢ Multi-query   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   similarity â”‚   (vector)   â”‚   expansion     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Top-K      â”‚ â€¢ Sparse     â”‚ â€¢ LLM variants  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Direct     â”‚   (BM25)     â”‚ â€¢ Vote          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   return     â”‚ â€¢ RRF fusion â”‚   aggregation   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚              â”‚ â€¢ Cross-     â”‚ â€¢ Cross-        â”‚  â”‚  â”‚
â”‚  â”‚  â”‚              â”‚   encoder    â”‚   encoder       â”‚  â”‚  â”‚
â”‚  â”‚  â”‚              â”‚   reranking  â”‚   reranking     â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Answer Generator                                     â”‚  â”‚
â”‚  â”‚  â€¢ Adaptive prompting based on query type            â”‚  â”‚
â”‚  â”‚  â€¢ Source citation enforcement                       â”‚  â”‚
â”‚  â”‚  â€¢ Hallucination prevention                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA ACCESS LAYER                         â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  Vector Store    â”‚  â”‚  Sparse Index    â”‚                â”‚
â”‚  â”‚  (ChromaDB)      â”‚  â”‚  (BM25)          â”‚                â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚                â”‚
â”‚  â”‚ â€¢ 768-dim        â”‚  â”‚ â€¢ Term frequency â”‚                â”‚
â”‚  â”‚   embeddings     â”‚  â”‚ â€¢ Keyword match  â”‚                â”‚
â”‚  â”‚ â€¢ HNSW index     â”‚  â”‚ â€¢ Fast lookup    â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Multi-Layer Cache Manager                    â”‚  â”‚
â”‚  â”‚         (cache_and_monitoring.py)                    â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  â€¢ Embedding Cache (10K entries, 1hr TTL)            â”‚  â”‚
â”‚  â”‚  â€¢ Query Cache (1K entries, 1hr TTL)                 â”‚  â”‚
â”‚  â”‚  â€¢ Result Cache (2K entries, 1hr TTL)                â”‚  â”‚
â”‚  â”‚  â€¢ Thread-safe LRU with TTL enforcement              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCUMENT PROCESSING LAYER                 â”‚
â”‚  (document_processor.py)                                    â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Document Loader (Parallel Processing)               â”‚  â”‚
â”‚  â”‚  â€¢ PDF (PyPDF + Tesseract OCR fallback)              â”‚  â”‚
â”‚  â”‚  â€¢ DOCX/DOC (Docx2txt)                               â”‚  â”‚
â”‚  â”‚  â€¢ TXT (Multi-encoding detection)                    â”‚  â”‚
â”‚  â”‚  â€¢ Markdown (Structured parsing)                     â”‚  â”‚
â”‚  â”‚  â€¢ ThreadPoolExecutor (4 workers)                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Intelligent Chunker                                  â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Strategies:                                          â”‚  â”‚
â”‚  â”‚  â€¢ Recursive: Hierarchical splitting (default)       â”‚  â”‚
â”‚  â”‚  â€¢ Semantic: Embedding-based boundaries (GPU)        â”‚  â”‚
â”‚  â”‚  â€¢ Sentence: Fixed sentence count                    â”‚  â”‚
â”‚  â”‚  â€¢ Hybrid: Recursive + semantic refinement (best)    â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Metadata Enrichment:                                 â”‚  â”‚
â”‚  â”‚  â€¢ File metadata (name, type, hash, size)            â”‚  â”‚
â”‚  â”‚  â€¢ Chunk position (index, page, total)               â”‚  â”‚
â”‚  â”‚  â€¢ Processing metadata (strategy, timestamp)         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI MODELS LAYER (Ollama)                  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LLM: llama3.1:8b                                     â”‚  â”‚
â”‚  â”‚  â€¢ 8B parameters                                      â”‚  â”‚
â”‚  â”‚  â€¢ 4096 token context window (optimized)             â”‚  â”‚
â”‚  â”‚  â€¢ Temperature: 0.0 (deterministic)                  â”‚  â”‚
â”‚  â”‚  â€¢ Full GPU layer loading                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Embeddings: nomic-embed-text                        â”‚  â”‚
â”‚  â”‚  â€¢ 768-dimensional vectors                           â”‚  â”‚
â”‚  â”‚  â€¢ 8192 token context                                â”‚  â”‚
â”‚  â”‚  â€¢ GPU-accelerated via Ollama                        â”‚  â”‚
â”‚  â”‚  â€¢ Batch size: 32                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2     â”‚  â”‚
â”‚  â”‚  â€¢ Cross-attention scoring                           â”‚  â”‚
â”‚  â”‚  â€¢ GPU-accelerated (CUDA)                            â”‚  â”‚
â”‚  â”‚  â€¢ Batch processing (16 pairs/batch)                 â”‚  â”‚
â”‚  â”‚  â€¢ ~10x faster than CPU                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Details

### 1. Adaptive Retrieval Engine

**Purpose**: Automatically select optimal retrieval strategy based on query complexity.

**Algorithm**: Multi-factor query classification
- **Length scoring**: Word count â†’ complexity correlation
- **Question type detection**: Who/what/why patterns
- **Complexity indicators**: Boolean operators, multiple questions

**Strategies**:

1. **Simple (score â‰¤ 1)**
   - Input: Factual queries ("Who is X?")
   - Method: Pure vector similarity search
   - Output: Top 3 chunks, normalized scores

2. **Hybrid (score â‰¤ 3)**
   - Input: Moderate queries ("What are the requirements?")
   - Method: RRF fusion of dense + sparse retrieval
   - Process:
     1. Dense retrieval (vector similarity, K=20)
     2. Sparse retrieval (BM25, K=20)
     3. RRF score calculation: `score = Î£(1 / (k + rank))`
     4. Cross-encoder reranking on GPU
     5. Weighted fusion: `final = 0.3*RRF + 0.7*rerank`
   - Output: Top 5 reranked chunks

3. **Advanced (score > 3)**
   - Input: Complex queries ("Compare X and Y")
   - Method: Multi-query expansion with consensus
   - Process:
     1. LLM generates 2 query variants
     2. Search with original + variants (K=15 each)
     3. Vote aggregation (chunks in multiple results ranked higher)
     4. Cross-encoder reranking on GPU
   - Output: Top 5 consensus chunks

### 2. Conversation Memory System ðŸ†•

**Purpose**: Maintain multi-turn conversation context for natural follow-up questions.

**Architecture**:
- **Sliding Window**: FIFO queue storing last 10 exchanges (deque with maxlen)
- **Exchange Storage**: ConversationExchange dataclass with query, response, retrieved_docs, query_type, strategy_used, timestamp
- **Automatic Summarization**: LLM-based compression triggered every 5 exchanges
- **Thread Safety**: RLock for concurrent access

**Key Features**:

1. **Follow-Up Detection**
   - Pattern matching: "that", "those", "this", "tell me more", "what about"
   - Short query heuristic: <10 words + history = likely follow-up
   - Returns: boolean indicating if current query references previous context

2. **Context Enhancement**
   - Method: `get_relevant_context_for_query(query, max_exchanges=5)`
   - Returns: (enhanced_query, relevant_documents)
   - Enhanced query includes conversation summary + recent exchanges
   - Relevant documents collected from previous turns

3. **Automatic Summarization**
   - Trigger: Every 5 exchanges (configurable)
   - Method: LLM generates <200 word summary preserving key topics
   - Compression: Typical 70-80% reduction in context size
   - Cumulative: New summaries append to existing summary

4. **Statistics Tracking**
   - Total exchanges, current window size, summarizations performed
   - Memory usage tracking
   - Hit/miss metrics for follow-up detection

**Integration Points**:
- app.py query() method: Automatic context enhancement for follow-ups
- app.py clear_conversation(): Manual memory reset
- Web interface: "Clear Chat" button triggers memory reset

**Performance Characteristics**:
- Follow-up detection: <10ms
- Context retrieval: <50ms
- Summarization: ~1-2 seconds (async, non-blocking)
- Memory per exchange: ~2KB
- Max memory (10 exchanges + summary): ~25KB

### 3. Explainability System ðŸ†•

**Purpose**: Provide transparent reasoning for query classification and retrieval strategy selection.

**Architecture**:
- **QueryExplanation Dataclass**: Structured explanation object with all decision factors
- **Factory Function**: `create_query_explanation()` auto-generates explanations
- **Integration**: Explanation objects flow through retrieval pipeline transparently

**Key Components**:

1. **Explanation Data Structure**
   ```python
   @dataclass
   class QueryExplanation:
       query_type: str              # Classification result
       complexity_score: int        # Total score from multi-factor analysis
       scoring_breakdown: Dict      # Factor â†’ contribution mapping
       thresholds_used: Dict        # Classification threshold values
       strategy_selected: str       # Retrieval strategy chosen
       strategy_reasoning: str      # Human-readable rationale
       key_factors: List[str]       # Primary contributing factors
       example_text: str            # Auto-generated explanation
   ```

2. **Explanation Generation**
   - **Scoring Breakdown**: Captures each classification factor's contribution
     - Length scoring: "12 words (+2)"
     - Question type: "why (+3)"
     - Complexity indicators: "has_and_operator=yes (+1)"

   - **Strategy Mapping**: Automatic strategy selection reasoning
     - simple â†’ "Straightforward query requires only dense vector retrieval"
     - moderate â†’ "Moderate complexity benefits from hybrid BM25+dense with reranking"
     - complex â†’ "High complexity requires query expansion and advanced fusion"

   - **Key Factors Extraction**: Identifies factors that contributed points (+ values)

   - **Human-Readable Text**: Auto-generated explanation combining all factors
     ```
     Query classified as COMPLEX (score: 5) because: length=12 words (+2),
     question_type=why (+3), has_and_operator=yes (+1). Thresholds: simpleâ‰¤1,
     moderateâ‰¤3. Using advanced_expanded strategy. Reasoning: High complexity
     requires query expansion and advanced fusion
     ```

3. **Serialization Support**
   - `to_dict()`: Convert to dictionary for JSON storage/logging
   - `from_dict()`: Restore from dictionary
   - Enables audit logging, debugging, and analytics

**Integration Points**:
- `adaptive_retrieval.py _classify_query()`: Generates explanation during classification
- `RetrievalResult`: Includes explanation object
- `app.py query()`: Logs explanation for audit trail
- `web_interface.py`: Optional UI display (collapsible section)

**Use Cases**:
- **User Transparency**: Show users why they got specific results
- **Developer Debugging**: Diagnose classification issues, tune thresholds
- **Audit Compliance**: Log AI decision reasoning for government/enterprise requirements
- **System Optimization**: Analyze explanation patterns to improve classification

**Performance Characteristics**:
- Explanation generation: <1ms (negligible overhead)
- Memory per explanation: ~500 bytes
- Serialization: <1ms for JSON conversion
- Zero impact on query latency

**Testing**:
- Unit tests: `test_explainability.py` (12 test cases)
- Covers: initialization, serialization, factory function, text generation
- Edge cases: empty breakdowns, missing fields, round-trip conversion

### 4. Feedback System ðŸ†•

**Purpose**: Enable continuous improvement through user feedback collection and automated performance analysis.

**Architecture**:
- **FeedbackManager Class**: Central feedback coordinator with JSON storage
- **Binary Rating System**: Simple thumbs up/down interface for user feedback
- **Automatic Tracking**: Query metadata captured transparently (type, strategy, answer, timestamp)
- **Analytics Engine**: Real-time statistics computation and trend analysis
- **Persistent Storage**: JSON-based feedback database (feedback.json)

**Key Components**:

1. **Feedback Collection**
   - **User Interface**: Thumbs up/down buttons in web UI after each answer
   - **Automatic Capture**: System stores complete context:
     - Query text
     - Generated answer
     - Rating (thumbs_up / thumbs_down)
     - Query type (simple/moderate/complex)
     - Strategy used (simple_dense/hybrid_reranked/advanced_expanded)
     - Timestamp (ISO format)
   - **Non-blocking**: <1ms overhead, asynchronous to query processing

2. **Analytics Dashboard**
   - **Overall Satisfaction Rate**: Percentage of positive feedback
   - **By Query Type**: Performance breakdown for simple/moderate/complex
   - **By Retrieval Strategy**: Effectiveness comparison across strategies
   - **Low-Rated Query Analysis**: Automatic identification of problematic queries
   - **Trend Tracking**: Historical performance over time

3. **Data Structure**
   ```python
   {
       "query": str,              # Original user query
       "answer": str,             # System-generated answer
       "rating": str,             # "thumbs_up" or "thumbs_down"
       "query_type": str,         # "simple", "moderate", or "complex"
       "strategy_used": str,      # Retrieval strategy identifier
       "timestamp": str           # ISO 8601 timestamp
   }
   ```

4. **Analytics API**
   - `add_feedback()`: Record new feedback entry
   - `get_feedback_stats()`: Compute overall statistics
   - `get_low_rated_queries()`: Retrieve queries with negative feedback
   - `get_recent_feedback()`: Access most recent N entries
   - All methods thread-safe with file locking

**Integration Points**:
- `app.py submit_feedback()`: Capture feedback from web UI
- `app.py query()`: Stores last query metadata for feedback correlation
- `web_interface.py`: Thumbs up/down buttons and admin stats viewer
- Performance monitoring: Combines satisfaction metrics with latency/throughput

**Use Cases**:
- **Continuous Improvement**: Identify underperforming strategies or query types
- **Data-Driven Tuning**: Adjust thresholds based on user satisfaction patterns
- **Quality Assurance**: Track satisfaction rate as key performance indicator
- **User Engagement**: Users feel their input shapes system behavior
- **Root Cause Analysis**: Investigate why specific queries receive poor ratings

**Performance Characteristics**:
- Feedback submission: <1ms (non-blocking, async file write)
- Statistics computation: ~10ms for 100 entries
- Storage overhead: ~500 bytes per feedback entry
- Memory footprint: Negligible (loaded on-demand)
- Scalability: Handles 10K+ entries efficiently

**Analytics Examples**:

*Overall Satisfaction*:
```
Total Feedback: 45 ratings
Thumbs Up: 32 (71%)
Thumbs Down: 13 (29%)
Satisfaction Rate: 71%
```

*By Query Type*:
```
Simple:    88% satisfaction (15/17) âœ… Excellent
Moderate:  71% satisfaction (12/17) ðŸ‘ Good
Complex:   45% satisfaction (5/11)  âš ï¸ Needs improvement
```

*By Strategy*:
```
simple_dense:       88% satisfaction (15/17) âœ…
hybrid_reranked:    73% satisfaction (11/15) ðŸ‘
advanced_expanded:  46% satisfaction (6/13)  âš ï¸
```

**Feedback-Driven Improvements**:
1. **Threshold Tuning**: Adjust classification thresholds based on satisfaction patterns
2. **Strategy Optimization**: Identify and improve underperforming retrieval strategies
3. **Query Expansion Refinement**: Tune LLM prompts for better query variants
4. **K-Value Adjustment**: Optimize result count based on user preferences

**Integration with Monitoring**:
```
Combined Analysis: Performance Metrics + User Feedback

Latency (P95): 3.2s         | Satisfaction: 45%  âš ï¸
Query Type: Complex         | Strategy: advanced_expanded
Bottleneck: Query expansion | Action: Optimize expansion prompts
```

**Testing**:
- Integration tests: `test_feedback_integration.py` (21 test cases)
- Covers: persistence, statistics, analytics, large datasets, edge cases
- Multi-instance testing: Verify data consistency across manager instances
- Performance testing: Large dataset handling (100+ entries)

**Future Enhancements**:
- Text comments with ratings (detailed feedback)
- Issue tagging (inaccurate, incomplete, irrelevant)
- Automated retraining based on low-rated queries
- A/B testing framework for strategy comparison
- User profiles for personalized optimization

### 5. Document Processing Pipeline

**Parallel Loading**:
- ThreadPoolExecutor with 4 workers
- Per-document error isolation
- Multi-format support (PDF, DOCX, TXT, MD)
- OCR fallback for scanned PDFs

**Chunking Strategies**:

1. **Recursive** (Default): Hierarchical splitting with semantic separators
2. **Semantic**: Sentence embeddings + boundary detection (GPU-accelerated)
3. **Hybrid**: Recursive â†’ semantic refinement for large chunks (Best balance)

**Metadata Enrichment**:
- File identification: name, type, hash, size
- Position tracking: chunk_index, page_number, total_chunks
- Processing info: strategy used, timestamp

### 6. Caching Infrastructure

**Three-Layer LRU Cache**:

1. **Embedding Cache** (10K entries, 1hr TTL)
   - Purpose: Cache expensive embedding computations
   - Key: MD5(text)
   - Benefit: ~90% hit rate on repeated content

2. **Query Cache** (1K entries, 1hr TTL)
   - Purpose: Cache complete query results
   - Key: MD5(query + parameters)
   - Benefit: Instant responses for duplicate queries

3. **Result Cache** (2K entries, 1hr TTL)
   - Purpose: Cache retrieval results before generation
   - Key: MD5(query)
   - Benefit: Fast re-generation with different prompts

**Features**:
- Thread-safe with RLock
- Automatic TTL expiration
- LRU eviction when full
- Hit/miss/eviction statistics

### 7. Performance Monitoring

**PyTorch-Based GPU Monitoring** (Docker-compatible):
- GPU utilization (estimated from memory reservation)
- VRAM usage (allocated + reserved in MB)
- CPU utilization (psutil)
- No nvidia-smi dependency

**Metrics Collection**:
- Query count & latency (avg, p50, p95, p99)
- Retrieval stage timing
- Cache hit rates
- Error rates
- Real-time updates (1 second interval)

### 8. Evaluation Framework

**Automated Testing** (evaluate.py):

1. **Retrieval Quality**
   - Precision, Recall, F1 score
   - Mean Reciprocal Rank (MRR)
   - By-difficulty breakdown

2. **Answer Quality**
   - Answer length statistics
   - Source citation count
   - Query type distribution

3. **Performance Metrics**
   - Cold/warm latency comparison
   - Cache effectiveness measurement
   - Percentile analysis

4. **Stress Testing**
   - Concurrent query handling
   - Throughput measurement (QPS)
   - Error rate under load

**Grading System**: A (90-100) â†’ F (<60) with automated recommendations

---

## Technology Stack

### Core Framework
- **Language**: Python 3.11
- **Web UI**: Gradio (chat interface + monitoring dashboard)
- **Async**: asyncio (concurrent operations)
- **Parallelism**: ThreadPoolExecutor (document loading)

### AI/ML Stack
- **LLM Platform**: Ollama (local inference)
- **Models**:
  - LLM: llama3.1:8b (8B parameter model)
  - Embeddings: nomic-embed-text (768-dim)
  - Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2
- **GPU**: CUDA 12.1+ (PyTorch backend)
- **Frameworks**: LangChain, sentence-transformers

### Data Layer
- **Vector DB**: ChromaDB (HNSW index)
- **Sparse Index**: BM25 (keyword matching)
- **Caching**: In-memory LRU caches

### Document Processing
- **PDF**: PyPDF + Tesseract OCR (fallback)
- **DOCX**: Docx2txt
- **Text**: Multi-encoding detection
- **Markdown**: Unstructured

### Infrastructure
- **Containerization**: Docker + Docker Compose
- **GPU Passthrough**: NVIDIA Container Toolkit
- **Networking**: Bridge network for inter-service communication
- **Volumes**: Persistent storage for documents, database, logs

---

## Integration Points

### Internal Communication
1. **App â†” Adaptive Retriever**: Query submission, retrieval results
2. **Adaptive Retriever â†” Data Layer**: Vector search, BM25 search
3. **App â†” Cache Manager**: Query/result caching
4. **App â†” Performance Monitor**: Metrics collection
5. **Web Interface â†” App**: User queries, status updates

### External Dependencies
1. **Ollama API** (port 11434): LLM inference, embeddings
2. **ChromaDB**: Vector storage and similarity search
3. **Docker Network**: Service-to-service communication

---

## Deployment Architecture

```
Docker Compose Services:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama Service    â”‚
â”‚   Port: 11434       â”‚
â”‚   GPU: NVIDIA       â”‚
â”‚   Models:           â”‚
â”‚   - llama3.1:8b     â”‚
â”‚   - nomic-embed-textâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    Docker Bridge Network
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RAG App Service   â”‚
â”‚   Port: 7860        â”‚
â”‚   GPU: NVIDIA       â”‚
â”‚   Runtime: nvidia   â”‚
â”‚   Volumes:          â”‚
â”‚   - ./documents     â”‚
â”‚   - ./chroma_db     â”‚
â”‚   - ./logs          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features**:
- Health checks on Ollama before RAG app starts
- GPU passthrough to both services
- Persistent volumes for data
- Automatic restart on failure

---

## Data Flow

### Query Processing Flow

```
1. User submits query via Gradio
   â†“
2. App.py receives query
   â†“
3. Check query cache â†’ [HIT] Return cached result â†’ END
   â†“ [MISS]
4. AdaptiveRetriever.retrieve(query)
   â†“
5. Classify query (simple/moderate/complex)
   â†“
6. Route to appropriate strategy:
   â”œâ”€ Simple: Vector search â†’ Top 3
   â”œâ”€ Hybrid: Vector + BM25 â†’ RRF â†’ Rerank â†’ Top 5
   â””â”€ Advanced: Multi-query â†’ Vote â†’ Rerank â†’ Top 5
   â†“
7. RetrievalResult (documents + scores + metadata)
   â†“
8. AdaptiveAnswerGenerator.generate(query, result)
   â†“
9. Build context with source citations
   â†“
10. LLM generates answer with adaptive prompting
    â†“
11. Format response with sources and metadata
    â†“
12. Cache result
    â†“
13. Return to user
```

### Document Indexing Flow

```
1. Place documents in documents/ folder
   â†“
2. Run index_documents.py
   â†“
3. DocumentProcessor.process_documents()
   â†“
4. DocumentLoader loads files in parallel (ThreadPoolExecutor)
   â†“
5. For each document:
   â”œâ”€ Detect format (PDF/DOCX/TXT/MD)
   â”œâ”€ Extract text (OCR fallback for PDFs)
   â”œâ”€ Enrich metadata (hash, size, timestamp)
   â””â”€ Create Document objects
   â†“
6. IntelligentChunker.chunk(documents)
   â”œâ”€ Apply selected strategy (recursive/semantic/hybrid)
   â”œâ”€ Add chunk metadata (index, position, strategy)
   â””â”€ Validate chunks (size, content quality)
   â†“
7. Generate embeddings (Ollama, batched)
   â†“
8. Store in ChromaDB (vector database)
   â†“
9. Save metadata for BM25 (JSON file)
   â†“
10. Indexing complete â†’ System ready
```

---

## Configuration Management

**Primary Config**: `config.yml`
- Structured YAML configuration
- Environment variable overrides via `RAG_*` prefix
- Pydantic validation with type checking
- Default values for all settings

**Key Configuration Sections**:
1. **LLM Settings**: Model, temperature, context window
2. **Embedding Settings**: Model, dimension, batch size
3. **Chunking**: Strategy, size, overlap
4. **Retrieval**: K-values, fusion method, thresholds
5. **Caching**: TTL, max sizes, enable/disable flags
6. **Performance**: Workers, batching, async settings
7. **Monitoring**: Logging, metrics, tracing

---

## Security Considerations

**Current Implementation**:
- Local deployment (no external API calls)
- Docker network isolation
- Volume-based data persistence

**Future Enhancements** (Roadmap):
- Document-level access control
- Query audit logging
- PII detection in documents
- Encrypted embeddings for sensitive data
- Answer provenance tracking

---

## Performance Characteristics

**Expected Latency**:
- Simple queries: 1-2s
- Hybrid queries: 2-4s
- Advanced queries: 4-6s
- Cached queries: <100ms

**Throughput**:
- Concurrent: ~10-15 QPS (with caching)
- Cold: ~2-3 QPS

**Resource Usage**:
- CPU RAM: 4-6 GB
- VRAM: 6-8 GB (llama3.1:8b)
- Disk: ~10 GB (models + indexes)

**Optimization Strategies**:
- Reduced context window (8192â†’4096 tokens)
- Smaller chunks (800â†’500 chars)
- Fewer rerank candidates (20â†’15)
- Multi-layer caching
- GPU acceleration throughout
- Batch processing (16 docs/batch)

---

## Failure Modes & Recovery

**Document Processing Failures**:
- Per-document error isolation
- OCR fallback for unreadable PDFs
- Multi-encoding detection for text files
- Graceful degradation (skip failed files)

**Retrieval Failures**:
- Strategy fallback (advancedâ†’hybridâ†’simple)
- Empty result handling (informative messages)
- Timeout protection (async with timeouts)

**LLM Failures**:
- Retry logic (automatic)
- Fallback to cached results
- Error messages to user

**Infrastructure Failures**:
- Docker restart policies (unless-stopped)
- Health checks (Ollama service)
- Volume persistence (data survives restarts)

---

## Extension Points

**Adding New Document Formats**:
- Extend `document_processor.py`
- Register in `supported_extensions` dict
- Implement loader method

**Custom Retrieval Strategies**:
- Add to `adaptive_retrieval.py`
- Define classification rules
- Implement retrieval method

**Alternative LLMs**:
- Change model in `config.yml`
- Ensure Ollama has model pulled
- Adjust context window as needed

**Custom Chunking Strategies**:
- Extend `IntelligentChunker` class
- Implement chunk method
- Add to strategy selection

---

## Development Guidelines

**Code Quality Standards**:
- Type hints on all functions
- Comprehensive docstrings
- Error handling with logging
- Async/await for I/O operations
- Thread-safe shared state

**Testing Strategy**:
- Evaluation suite (evaluate.py)
- Manual testing via web UI
- Performance benchmarking
- Stress testing (concurrent queries)

**Version Control**:
- Git commits for each feature
- Semantic commit messages
- Feature branches for major changes

---

## Project Evolution

This system evolved through **multi-agent iterative development** (v2.5 â†’ v3.5.1):

### Milestone 1: Conversation Memory Enhancement âœ…
- **Developer**: hollowed_eyes (dev-001)
- **Tester**: zhadyz (ops-001)
- **Features**: Sliding window (10 exchanges), LLM summarization, follow-up detection
- **Impact**: Multi-turn context awareness, +50-100ms latency, ~20KB per conversation
- **Commits**: e92802d (dev), 0716bf5 (docs), 84dbec1 (state)

### Milestone 2: Explainability Features âœ…
- **Developer**: hollowed_eyes (dev-002)
- **Tester**: zhadyz (ops-002)
- **Features**: QueryExplanation dataclass, scoring breakdown, strategy reasoning
- **Impact**: Transparent AI decisions, <1ms overhead, compliance-ready
- **Commits**: fb6e143 (dev), 99996cc (docs)

### Milestone 3: User Feedback System âœ…
- **Developer**: hollowed_eyes (dev-003)
- **Tester**: zhadyz (ops-003)
- **Features**: Thumbs up/down, analytics dashboard, satisfaction tracking
- **Impact**: Continuous learning, data-driven optimization, 71% satisfaction baseline
- **Commits**: 044264c (dev), 7f91332 (docs), 326ed4e (state)

### Milestone 4: Portfolio Finalization âœ…
- **Owner**: zhadyz (ops-004)
- **Deliverables**: IMPROVEMENTS.md, DEMO_SCRIPT.md, portfolio highlights, badges
- **Impact**: Production-ready documentation, demo-ready presentation

### Development Method

**Multi-Agent Coordination**:
- **medicant_bias** (Architect): Designed roadmap, defined milestones, established coordination protocol
- **hollowed_eyes** (Engineer): Implemented features, wrote unit tests, committed incrementally
- **zhadyz** (Tester/Docs): Created integration tests (63 total), comprehensive documentation, portfolio polish

**Coordination Protocol**:
- state.json: Central task ownership and handoff tracking
- Redis pub/sub: Real-time event-driven coordination
- Git strategy: Clean history showing iterative progression

**Code Metrics**:
- Total LOC added: ~1,300 lines
- Integration tests: 63 tests across 3 test suites
- Documentation: 3 major docs + 3 example files
- Commits: 9 (from baseline to v3.5.1)

### Roadmap (Future)

**Planned Enhancements**:
1. âœ… Multi-turn conversation memory (Milestone 1)
2. âœ… Explainability (query classification reasoning) (Milestone 2)
3. âœ… Feedback loops (user ratings, learning) (Milestone 3)
4. â³ Multi-modal support (tables, figures)
5. â³ Security hardening (audit logs, access control)

**See**: `IMPROVEMENTS.md` for detailed evolution timeline with git commits and technical analysis
**See**: `docs/DEMO_SCRIPT.md` for live demonstration guide

---

## Credits

**System Architecture**: medicant_bias
**Feature Implementation**: hollowed_eyes
**Testing & Documentation**: zhadyz
**Multi-Agent Framework**: LangGraph + Redis coordination
**Development Environment**: Claude Code

---

**Document Version**: 3.5.1
**Status**: Production-ready, portfolio-optimized
**Last Review**: 2025-10-11 (Milestone 4 complete)
