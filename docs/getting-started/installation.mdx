# Installation Guide

Get ATLAS Protocol up and running in minutes. This guide covers three installation methods: **Docker Compose** (recommended for production), **Manual Installation** (for development), and **Desktop App** (for end users).

---

## System Requirements

### Minimum Requirements (CPU Mode)
- **OS**: Linux (Ubuntu 20.04+), macOS (12+), Windows 10/11 with WSL2
- **RAM**: 16GB
- **Storage**: 20GB free space
- **CPU**: 4+ cores, x86_64 architecture
- **Software**: Docker 24.0+ & Docker Compose 2.20+

### Recommended Requirements (GPU Accelerated)
- **OS**: Linux with NVIDIA drivers, Windows 11 with WSL2 + NVIDIA drivers
- **RAM**: 32GB+
- **GPU**: NVIDIA GPU with 8GB+ VRAM (RTX 3060 or better)
  - Compute Capability 7.0+ (Volta, Turing, Ampere, Ada Lovelace, Hopper)
  - CUDA 12.1+ compatible
- **Storage**: 50GB SSD
- **CPU**: 8+ cores
- **NVIDIA Driver**: 535.86.10+ (Linux), 531.18+ (Windows)

### Production Requirements (Optimal)
- **RAM**: 64GB+ (96GB recommended)
- **GPU**: NVIDIA RTX 4080/4090 or RTX 5080 (16GB VRAM)
- **Storage**: 100GB+ NVMe SSD
- **CPU**: 12+ cores (Ryzen 9 / Intel i9)
- **Network**: 1Gbps+

---

## Quick Decision Matrix

| Use Case | Installation Method | GPU Required | Setup Time |
|----------|---------------------|--------------|------------|
| Production deployment | Docker Compose | Recommended | 10-15 min |
| Development/Testing | Manual Install | Optional | 20-30 min |
| End-user application | Desktop App | Optional | 5 min |
| Kubernetes cluster | Helm Chart | Recommended | 30-45 min |

---

## Method 1: Docker Compose (Recommended)

### Prerequisites
```bash
# Check Docker version (24.0+ required)
docker --version

# Check Docker Compose version (2.20+ required)
docker compose version

# Verify NVIDIA runtime (for GPU support)
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

<details>
<summary>Don't have Docker installed? Click here for installation</summary>

**Linux (Ubuntu/Debian)**
```bash
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER
newgrp docker
```

**Windows/Mac**
Download Docker Desktop from [docker.com](https://www.docker.com/products/docker-desktop/)

**NVIDIA Docker Runtime (for GPU support)**
```bash
# Linux only
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```
</details>

### Installation Steps

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/apollo-rag.git
cd apollo-rag
```

2. **Download required models**
```bash
# Create models directory
mkdir -p models

# Download Llama 3.1 8B Instruct (Q5_K_M quantization, 5.4GB)
wget -O models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf \
  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf

# Download draft model for speculative decoding (771MB)
wget -O models/Llama-3.2-1B-Instruct.Q4_K_M.gguf \
  https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf
```

<details>
<summary>Alternative: Download models manually</summary>

Visit [HuggingFace](https://huggingface.co/bartowski) and download:
- `Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf` (5.4GB)
- `Llama-3.2-1B-Instruct.Q4_K_M.gguf` (771MB)

Place them in the `models/` directory.
</details>

3. **Configure environment**
```bash
# Copy example configuration
cp backend/config.yml.example backend/config.yml

# Edit configuration for your hardware
nano backend/config.yml
```

**GPU Configuration (RTX 3060+)**
```yaml
llamacpp:
  model_path: "./models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf"
  n_gpu_layers: 33  # Offload all layers to GPU
  n_ctx: 8192       # Context window
  enable_speculative_decoding: true
  draft_model_path: "./models/Llama-3.2-1B-Instruct.Q4_K_M.gguf"
```

**CPU-Only Configuration**
```yaml
llamacpp:
  model_path: "./models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf"
  n_gpu_layers: 0   # CPU inference
  n_ctx: 4096       # Smaller context for RAM constraints
  n_threads: 8      # Match your CPU cores
  enable_speculative_decoding: false
```

4. **Build and start services**
```bash
# Build the Docker image (first time: ~10 minutes)
docker compose -f backend/docker-compose.atlas.yml build

# Start all services
docker compose -f backend/docker-compose.atlas.yml up -d

# View logs
docker compose -f backend/docker-compose.atlas.yml logs -f atlas-backend
```

5. **Verify installation**
```bash
# Check service health
curl http://localhost:8000/api/health

# Expected response:
# {"status":"healthy","version":"4.0.0","timestamp":"2025-01-27T12:00:00Z"}

# Test query endpoint
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Hello, are you working?",
    "mode": "simple",
    "use_context": false
  }'
```

### Monitoring & Management

```bash
# View all service status
docker compose -f backend/docker-compose.atlas.yml ps

# Access monitoring dashboards
# Grafana: http://localhost:3001 (admin / atlas_admin_2025)
# Prometheus: http://localhost:9090
# cAdvisor: http://localhost:8081

# Stop services
docker compose -f backend/docker-compose.atlas.yml down

# Stop and remove volumes (CAUTION: deletes all data)
docker compose -f backend/docker-compose.atlas.yml down -v
```

---

## Method 2: Manual Installation (Development)

### Prerequisites
```bash
# Python 3.11+
python3 --version

# Node.js 18+
node --version
npm --version

# System dependencies (Ubuntu/Debian)
sudo apt-get update && sudo apt-get install -y \
  build-essential \
  cmake \
  tesseract-ocr \
  tesseract-ocr-eng \
  poppler-utils \
  libglib2.0-0
```

### Backend Setup

1. **Clone and navigate**
```bash
git clone https://github.com/yourusername/apollo-rag.git
cd apollo-rag/backend
```

2. **Create Python virtual environment**
```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows
```

3. **Install Python dependencies**

<tabs>
<tab label="GPU (CUDA 12.1)">
```bash
# Install PyTorch with CUDA support
pip install torch==2.5.1 torchvision torchaudio \
  --index-url https://download.pytorch.org/whl/cu121

# Build llama-cpp-python with CUDA
CMAKE_ARGS="-DGGML_CUDA=ON" pip install llama-cpp-python==0.3.2

# Install remaining dependencies
pip install -r requirements.txt
```
</tab>

<tab label="CPU Only">
```bash
# Install PyTorch CPU
pip install torch==2.5.1 torchvision torchaudio \
  --index-url https://download.pytorch.org/whl/cpu

# Install llama-cpp-python (CPU)
pip install llama-cpp-python==0.3.2

# Install remaining dependencies
pip install -r requirements.txt
```
</tab>
</tabs>

4. **Start infrastructure services**
```bash
# Qdrant vector database
docker run -d -p 6333:6333 -p 6334:6334 \
  -v $(pwd)/data/qdrant:/qdrant/storage \
  qdrant/qdrant:v1.15.0

# Redis cache
docker run -d -p 6379:6379 \
  -v $(pwd)/data/redis:/data \
  redis:7.2-alpine redis-server --maxmemory 8gb
```

5. **Download models** (same as Docker method above)

6. **Configure and run**
```bash
# Copy configuration
cp config.yml.example config.yml

# Start backend
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### Frontend Setup

1. **Navigate to frontend**
```bash
cd ../  # Back to root
```

2. **Install dependencies**
```bash
npm install
```

3. **Configure API endpoint**
```bash
# Create .env file
echo "VITE_API_URL=http://localhost:8000" > .env
```

4. **Start development server**
```bash
npm run dev
```

Access the application at `http://localhost:5173`

---

## Method 3: Desktop Application (Tauri)

### Prerequisites
- **Rust**: 1.70+
- **Node.js**: 18+

### Installation

1. **Install Rust** (if not already installed)
```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
```

2. **Clone and build**
```bash
git clone https://github.com/yourusername/apollo-rag.git
cd apollo-rag

# Install Node dependencies
npm install

# Install Tauri CLI
npm install --save-dev @tauri-apps/cli

# Build desktop app (production)
npm run tauri:build
```

3. **Run desktop app**
```bash
# Development mode
npm run tauri:dev

# Or run the built executable
# Linux: ./src-tauri/target/release/apollo-rag
# Windows: .\src-tauri\target\release\apollo-rag.exe
# macOS: ./src-tauri/target/release/apollo-rag.app
```

---

## Verification & Testing

### Health Check
```bash
curl http://localhost:8000/api/health
```

**Expected Response:**
```json
{
  "status": "healthy",
  "version": "4.0.0",
  "components": {
    "llm": "ready",
    "vector_store": "ready",
    "cache": "ready"
  },
  "gpu_available": true,
  "timestamp": "2025-01-27T12:00:00Z"
}
```

### First Query Test
```bash
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is ATLAS Protocol?",
    "mode": "simple"
  }'
```

### Upload Document Test
```bash
curl -X POST http://localhost:8000/api/documents/upload \
  -F "file=@/path/to/document.pdf"
```

---

## Troubleshooting

### Issue: GPU not detected

**Symptoms:**
```
WARNING: GPU not available, falling back to CPU
```

**Solutions:**
```bash
# 1. Verify NVIDIA driver
nvidia-smi

# 2. Check Docker GPU access
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# 3. Verify CUDA in container
docker exec atlas-backend python3 -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# 4. Check llama.cpp GPU support
docker exec atlas-backend python3 -c "from llama_cpp import Llama; print('GPU support verified')"
```

### Issue: Out of Memory (OOM)

**Symptoms:**
```
RuntimeError: CUDA out of memory
```

**Solutions:**
1. Reduce GPU layers: `n_gpu_layers: 24` (instead of 33)
2. Use smaller quantization: `Q4_K_M` instead of `Q5_K_M`
3. Reduce context: `n_ctx: 4096` instead of 8192
4. Disable speculative decoding: `enable_speculative_decoding: false`

### Issue: Slow startup (>60 seconds)

**Causes & Solutions:**
- **Models not cached**: Ensure models are embedded in Docker image
- **Network slow**: Download models locally before building
- **CPU bottleneck**: Increase Docker CPU allocation
- **Disk I/O**: Use SSD, avoid network volumes

```bash
# Check startup time
docker logs atlas-backend 2>&1 | grep "startup_duration"
```

### Issue: Port already in use

```bash
# Find process using port 8000
lsof -i :8000  # Linux/Mac
netstat -ano | findstr :8000  # Windows

# Change port in docker-compose.yml
ports:
  - "8001:8000"  # Map to different host port
```

### Issue: Redis connection refused

```bash
# Check Redis container
docker ps | grep redis

# Test Redis connection
docker exec atlas-redis redis-cli ping
# Expected: PONG

# Restart Redis
docker compose -f backend/docker-compose.atlas.yml restart redis
```

### Issue: Qdrant vector store errors

```bash
# Check Qdrant health
curl http://localhost:6333/health

# View Qdrant logs
docker logs atlas-qdrant

# Reset Qdrant (CAUTION: deletes all vectors)
docker compose -f backend/docker-compose.atlas.yml down
docker volume rm atlas_qdrant_storage
docker compose -f backend/docker-compose.atlas.yml up -d
```

---

## Environment Variables Reference

### Core Settings
| Variable | Default | Description |
|----------|---------|-------------|
| `CONFIG_PATH` | `/app/config.yml` | Path to configuration file |
| `LLM_BACKEND` | `llamacpp` | LLM backend (llamacpp/ollama) |
| `VECTOR_STORE` | `qdrant` | Vector store (qdrant/chroma) |

### LLM Settings
| Variable | Default | Description |
|----------|---------|-------------|
| `MODEL_PATH` | - | Path to GGUF model file |
| `GPU_LAYERS` | `33` | Number of layers to offload to GPU |
| `CONTEXT_LENGTH` | `8192` | Maximum context window |
| `TEMPERATURE` | `0.7` | Sampling temperature |

### Performance Settings
| Variable | Default | Description |
|----------|---------|-------------|
| `WORKERS` | `4` | Uvicorn worker processes |
| `MAX_CONCURRENT_REQUESTS` | `100` | Max simultaneous requests |
| `REQUEST_TIMEOUT` | `120` | Request timeout (seconds) |

See [Configuration Guide](./configuration.mdx) for complete reference.

---

## Next Steps

‚úÖ **Installation complete!**

- üìñ [Quick Start Guide](./quickstart.mdx) - Run your first query
- ‚öôÔ∏è [Configuration Guide](./configuration.mdx) - Optimize for your hardware
- üöÄ [Deployment Guide](../deployment/docker.mdx) - Production deployment
- üìö [API Reference](../api/endpoints.mdx) - Integrate with your application

---

## Getting Help

- üìñ **Documentation**: [docs.apollo.onyxlab.ai](https://docs.apollo.onyxlab.ai)
- üí¨ **Discord**: [discord.gg/apollo-rag](https://discord.gg/apollo-rag)
- üêõ **Issues**: [GitHub Issues](https://github.com/yourusername/apollo-rag/issues)
- üìß **Email**: support@apollo.onyxlab.ai

---

**Last Updated**: January 2025 | **Version**: 4.0.0
