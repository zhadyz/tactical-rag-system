# Quick Start Guide

Get from zero to your first intelligent query in **5 minutes**. This guide assumes you've completed the [Installation](./installation.mdx).

---

## Prerequisites

‚úÖ ATLAS Protocol installed (Docker or manual)
‚úÖ Services running (`docker compose ps` shows all healthy)
‚úÖ Backend accessible at `http://localhost:8000`

---

## Step 1: Verify System Health (30 seconds)

```bash
# Check all services are running
curl http://localhost:8000/api/health
```

**Expected Response:**
```json
{
  "status": "healthy",
  "version": "4.0.0",
  "components": {
    "llm": "ready",
    "vector_store": "ready",
    "cache": "ready",
    "embedding_model": "ready"
  },
  "gpu_available": true,
  "gpu_layers_loaded": 33,
  "model_loaded": "Meta-Llama-3.1-8B-Instruct-Q5_K_M",
  "timestamp": "2025-01-27T12:00:00Z"
}
```

<Warning>
If `"status": "unhealthy"`, wait 30 seconds and try again. First startup takes 20-30 seconds as models load into memory.
</Warning>

---

## Step 2: Your First Query (1 minute)

### Using curl
```bash
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Hello! Can you introduce yourself?",
    "mode": "simple",
    "use_context": false
  }'
```

### Using Python
```python
import requests

response = requests.post(
    "http://localhost:8000/api/query",
    json={
        "question": "Hello! Can you introduce yourself?",
        "mode": "simple",
        "use_context": False
    }
)

result = response.json()
print(result["answer"])
```

### Using JavaScript (Node.js)
```javascript
const response = await fetch('http://localhost:8000/api/query', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    question: "Hello! Can you introduce yourself?",
    mode: "simple",
    use_context: false
  })
});

const result = await response.json();
console.log(result.answer);
```

**Expected Response:**
```json
{
  "answer": "I'm ATLAS Protocol, an advanced document intelligence system...",
  "sources": [],
  "metadata": {
    "mode": "simple",
    "query_type": "conversational",
    "processing_time_ms": 847,
    "tokens_per_second": 94.2,
    "gpu_accelerated": true
  },
  "timestamp": "2025-01-27T12:00:00Z"
}
```

---

## Step 3: Upload Your First Document (2 minutes)

### Prepare a Sample Document

Create a test file `company_policy.txt`:
```text
Company Remote Work Policy

Effective Date: January 1, 2025

1. Eligibility
All full-time employees are eligible for remote work after 90 days of employment.

2. Work Schedule
Remote employees must maintain core hours of 9 AM - 3 PM in their local timezone.

3. Equipment
The company provides a laptop and $500 annual stipend for home office equipment.

4. Communication
Daily check-ins via Slack are required. Weekly video meetings are mandatory.

5. Performance
Remote work privileges may be revoked if performance metrics decline.
```

### Upload via API
```bash
curl -X POST http://localhost:8000/api/documents/upload \
  -F "file=@company_policy.txt" \
  -F "metadata={\"category\":\"policy\",\"department\":\"HR\"}"
```

**Response:**
```json
{
  "document_id": "doc_abc123",
  "filename": "company_policy.txt",
  "status": "processing",
  "chunks_created": 3,
  "processing_time_ms": 1240
}
```

### Upload via Python
```python
import requests

with open("company_policy.txt", "rb") as f:
    files = {"file": f}
    metadata = {
        "metadata": '{"category":"policy","department":"HR"}'
    }

    response = requests.post(
        "http://localhost:8000/api/documents/upload",
        files=files,
        data=metadata
    )

print(response.json())
```

<Info>
Documents are automatically chunked, embedded, and indexed. Processing time: ~1-2 seconds per page.
</Info>

---

## Step 4: Query Your Document (1 minute)

### Simple Query (Direct Vector Search)
```bash
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is the remote work equipment stipend?",
    "mode": "simple",
    "use_context": false
  }'
```

**Response:**
```json
{
  "answer": "The company provides a $500 annual stipend for home office equipment to remote employees.",
  "sources": [
    {
      "content": "3. Equipment\nThe company provides a laptop and $500 annual stipend for home office equipment.",
      "metadata": {
        "filename": "company_policy.txt",
        "chunk_id": 2,
        "category": "policy"
      },
      "relevance_score": 0.94
    }
  ],
  "metadata": {
    "mode": "simple",
    "processing_time_ms": 623,
    "chunks_retrieved": 7,
    "chunks_reranked": 3,
    "cache_hit": false
  }
}
```

### Adaptive Query (Intelligent Routing)
```bash
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Can I work remotely if I just started?",
    "mode": "adaptive",
    "use_context": false
  }'
```

**Response includes reasoning:**
```json
{
  "answer": "No, you cannot work remotely immediately upon starting. According to the policy, all full-time employees are eligible for remote work only after 90 days of employment.",
  "sources": [...],
  "metadata": {
    "mode": "adaptive",
    "query_type": "eligibility_check",
    "strategy": "precise",
    "reasoning": "Query requires specific eligibility criteria lookup"
  },
  "explanation": {
    "analysis": "This is a specific eligibility question requiring precise policy lookup.",
    "strategy_selected": "precise",
    "confidence": 0.97
  }
}
```

---

## Step 5: Streaming Responses (Advanced)

For real-time token-by-token streaming:

### Using curl
```bash
curl -X POST http://localhost:8000/api/query/stream \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Summarize the remote work policy",
    "mode": "simple"
  }'
```

**Server-Sent Events Output:**
```
data: {"type":"token","content":"The"}
data: {"type":"token","content":" remote"}
data: {"type":"token","content":" work"}
data: {"type":"token","content":" policy"}
...
data: {"type":"sources","content":[...]}
data: {"type":"metadata","content":{...}}
data: {"type":"done"}
```

### Using JavaScript (EventSource)
```javascript
const eventSource = new EventSource(
  'http://localhost:8000/api/query/stream?' +
  new URLSearchParams({
    question: "Summarize the remote work policy",
    mode: "simple"
  })
);

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);

  if (data.type === 'token') {
    process.stdout.write(data.content);
  } else if (data.type === 'done') {
    console.log('\n\nStreaming complete!');
    eventSource.close();
  }
};
```

---

## Step 6: Conversation Context (Multi-Turn)

ATLAS Protocol maintains conversation memory for follow-up questions:

```bash
# First question
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What are the core work hours for remote employees?",
    "mode": "simple",
    "use_context": true
  }'

# Follow-up (uses context)
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Is that in local time or company time?",
    "mode": "simple",
    "use_context": true
  }'
```

**Second Response:**
```json
{
  "answer": "The core hours of 9 AM - 3 PM are in the employee's local timezone, not company time.",
  "context_used": true,
  "previous_query": "What are the core work hours for remote employees?"
}
```

### Clear Conversation Memory
```bash
curl -X POST http://localhost:8000/api/conversation/clear
```

---

## Understanding Query Modes

| Mode | Use Case | Speed | Accuracy | Best For |
|------|----------|-------|----------|----------|
| `simple` | Quick lookups | ‚ö° Fast (0.6-1.2s) | ‚≠ê‚≠ê‚≠ê‚≠ê | FAQs, direct questions |
| `adaptive` | Complex queries | üêå Slower (2-5s) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Analysis, reasoning, edge cases |

### When to use `simple`:
- "What is X?"
- "How much does Y cost?"
- "When is the deadline?"

### When to use `adaptive`:
- "Compare X and Y"
- "What if I...?"
- "Why does the policy say...?"

---

## Interactive Playground

### Web UI
Open `http://localhost:5173` in your browser for the full desktop interface:

1. **Chat Interface**: Type questions naturally
2. **Document Upload**: Drag & drop PDFs, DOCX, TXT
3. **Settings**: Adjust temperature, context length, reranking
4. **Metrics**: Real-time performance monitoring

### API Playground (Swagger)
Open `http://localhost:8000/docs` for interactive API documentation:

- Try endpoints directly in browser
- See request/response schemas
- Download OpenAPI spec

---

## Performance Expectations

### Query Latency (RTX 4080+, GPU mode)
- **Simple mode**: 600-1200ms
- **Adaptive mode**: 2000-5000ms
- **Streaming**: First token in 200-400ms

### Query Latency (CPU mode)
- **Simple mode**: 8000-15000ms
- **Adaptive mode**: 20000-40000ms
- **Streaming**: First token in 2000-4000ms

### Throughput
- **GPU**: 80-100 tokens/second
- **CPU**: 4-8 tokens/second

### Document Processing
- **Text extraction**: ~1 second/page
- **OCR (images)**: ~3 seconds/page
- **Embedding**: ~50ms per chunk
- **Indexing**: ~10ms per chunk

---

## Common Patterns

### 1. Batch Document Upload
```python
import os
import requests

docs_dir = "./documents"
for filename in os.listdir(docs_dir):
    if filename.endswith(('.pdf', '.txt', '.docx')):
        with open(f"{docs_dir}/{filename}", "rb") as f:
            requests.post(
                "http://localhost:8000/api/documents/upload",
                files={"file": f}
            )
        print(f"‚úì Uploaded {filename}")
```

### 2. Query with Retry Logic
```python
import requests
import time

def query_with_retry(question, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = requests.post(
                "http://localhost:8000/api/query",
                json={"question": question, "mode": "simple"},
                timeout=30
            )
            return response.json()
        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
            else:
                raise

result = query_with_retry("What is the policy?")
print(result["answer"])
```

### 3. Extract Sources Only
```python
response = requests.post(
    "http://localhost:8000/api/query",
    json={
        "question": "remote work eligibility",
        "mode": "simple"
    }
).json()

for source in response["sources"]:
    print(f"Source: {source['metadata']['filename']}")
    print(f"Relevance: {source['relevance_score']:.2f}")
    print(f"Content: {source['content']}\n")
```

---

## Troubleshooting

### Query Returns Empty Sources
**Cause**: No documents uploaded yet, or query doesn't match content.

**Solution**:
```bash
# List uploaded documents
curl http://localhost:8000/api/documents/list

# Upload sample documents
curl -X POST http://localhost:8000/api/documents/upload \
  -F "file=@sample.pdf"
```

### Slow Queries (>10 seconds)
**Cause**: CPU mode, or GPU not utilized.

**Check**:
```bash
# Verify GPU is active
curl http://localhost:8000/api/health | grep gpu_available

# Check GPU layers loaded
curl http://localhost:8000/api/health | grep gpu_layers
```

**Fix**: See [Configuration Guide](./configuration.mdx#gpu-optimization)

### Rate Limiting Errors
**Cause**: Exceeded 30 requests/minute limit.

**Response**:
```json
{
  "detail": "Rate limit exceeded. Max 30 requests per 60 seconds."
}
```

**Solution**: Implement backoff, or increase limit in `backend/app/api/query.py`:
```python
RATE_LIMIT_REQUESTS = 100  # Increase from 30
```

---

## Next Steps

üéâ **You're ready to build!**

- ‚öôÔ∏è [Configuration Guide](./configuration.mdx) - Fine-tune for your use case
- üöÄ [Deployment Guide](../deployment/docker.mdx) - Production setup
- üîß [Model Management](../advanced/model-management.mdx) - Swap models, quantization
- üìö [API Reference](../api/endpoints.mdx) - Complete endpoint documentation

---

## Video Tutorial

<Video src="/tutorials/quickstart-5min.mp4" />

**Recording Guide** (for dev team):
1. Show clean terminal, run `docker compose ps`
2. Demonstrate health check: `curl http://localhost:8000/api/health`
3. Run first query with JSON pretty-print
4. Upload `company_policy.txt` with visible terminal output
5. Query the document, show relevant sources highlighted
6. Open Web UI, demonstrate drag-drop upload
7. Show streaming response in real-time
8. End with Grafana dashboard showing metrics

**Duration**: 4-5 minutes
**Resolution**: 1920x1080
**Voiceover**: Optional (can be silent with text overlays)

---

**Last Updated**: January 2025 | **Version**: 4.0.0
